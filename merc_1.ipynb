{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator,TransformerMixin, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "import xgboost as xgb\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.linear_model import ElasticNetCV, LassoLarsCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD,NMF\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.manifold import TSNE, MDS,Isomap\n",
    "\n",
    "\n",
    "\n",
    "class StackingEstimator(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        self.estimator.fit(X, y, **fit_params)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = check_array(X)\n",
    "        X_transformed = np.copy(X)\n",
    "        # add class probabilities as a synthetic feature\n",
    "        if issubclass(self.estimator.__class__, ClassifierMixin) and hasattr(self.estimator, 'predict_proba'):\n",
    "            X_transformed = np.hstack((self.estimator.predict_proba(X), X))\n",
    "\n",
    "        # add class prodiction as a synthetic feature\n",
    "        X_transformed = np.hstack((np.reshape(self.estimator.predict(X), (-1, 1)), X_transformed))\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "for c in train.columns:\n",
    "    if train[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(train[c].values) + list(test[c].values))\n",
    "        train[c] = lbl.transform(list(train[c].values))\n",
    "        test[c] = lbl.transform(list(test[c].values))\n",
    "\n",
    "# train = train.T.drop_duplicates().T\n",
    "# test = test.T.drop_duplicates().T\n",
    "\n",
    "\n",
    "n_comp = 20\n",
    "\n",
    "# tSVD\n",
    "tsvd = TruncatedSVD(n_components=n_comp, random_state=420)\n",
    "tsvd_results_train = tsvd.fit_transform(train.drop([\"y\"], axis=1))\n",
    "tsvd_results_test = tsvd.transform(test)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=n_comp, random_state=420)\n",
    "pca2_results_train = pca.fit_transform(train.drop([\"y\"], axis=1))\n",
    "pca2_results_test = pca.transform(test)\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=420)\n",
    "ica2_results_train = ica.fit_transform(train.drop([\"y\"], axis=1))\n",
    "ica2_results_test = ica.transform(test)\n",
    "\n",
    "# GRP\n",
    "grp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\n",
    "grp_results_train = grp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "grp_results_test = grp.transform(test)\n",
    "\n",
    "# SRP\n",
    "srp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\n",
    "srp_results_train = srp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "srp_results_test = srp.transform(test)\n",
    "\n",
    "# NMF\n",
    "nmf = NMF(n_components=n_comp, init=None, solver='cd', tol=0.0001, max_iter=200, random_state=420, alpha=0.0, \n",
    "          l1_ratio=0.0, verbose=0, shuffle=False, nls_max_iter=2000, sparseness=None, beta=1, eta=0.1)\n",
    "nmf_results_train = nmf.fit_transform(train.drop([\"y\"], axis=1))\n",
    "nmf_results_test = nmf.transform(test)\n",
    "\n",
    "# NMF\n",
    "tsne = TSNE(n_components=5,  random_state=420)\n",
    "tsne_results_train = tsne.fit_transform(train.drop([\"y\"], axis=1))\n",
    "tsne_results_test = tsne.fit_transform(test)\n",
    "\n",
    "#MDS\n",
    "mds = MDS(n_components=5,  random_state=420)\n",
    "mds_results_train = mds.fit_transform(train.drop([\"y\"], axis=1))\n",
    "mds_results_test = mds.fit_transform(test)\n",
    "\n",
    "ism = Isomap(n_components=n_comp)\n",
    "ism_results_train = ism.fit_transform(train.drop([\"y\"], axis=1))\n",
    "ism_results_test = ism.transform(test)\n",
    "\n",
    "\n",
    "#save columns list before adding the decomposition components\n",
    "\n",
    "usable_columns = list(set(train.columns) - set(['y']))\n",
    "\n",
    "# Append decomposition components to datasets\n",
    "for i in range(1, n_comp + 1):\n",
    "    train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n",
    "    test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n",
    "\n",
    "    train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n",
    "    test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n",
    "\n",
    "    train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n",
    "    test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n",
    "\n",
    "    train['grp_' + str(i)] = grp_results_train[:, i - 1]\n",
    "    test['grp_' + str(i)] = grp_results_test[:, i - 1]\n",
    "\n",
    "    train['srp_' + str(i)] = srp_results_train[:, i - 1]\n",
    "    test['srp_' + str(i)] = srp_results_test[:, i - 1]\n",
    "\n",
    "    train['nmf_' + str(i)] = nmf_results_train[:, i - 1]\n",
    "    test['nmf_' + str(i)] = nmf_results_test[:, i - 1]\n",
    "\n",
    "    train['ism_' + str(i)] = ism_results_train[:, i - 1]\n",
    "    test['ism_' + str(i)] = ism_results_test[:, i - 1]\n",
    "\n",
    "#     train['mds_' + str(i)] = mds_results_train[:, i - 1]\n",
    "#     test['mds_' + str(i)] = mds_results_test[:, i - 1]\n",
    "\n",
    "#     train['mds_' + str(i)] = mds_results_train[:, i - 1]\n",
    "#     test['mds_' + str(i)] = mds_results_test[:, i - 1]\n",
    "\n",
    "#usable_columns = list(set(train.columns) - set(['y']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4209, 517)\n",
      "(4209, 517)\n",
      "[0]\ttrain-error:67.337+1.00852\ttrain-rmse:4.13627+0.000710392\ttest-error:68.5428+7.70445\ttest-rmse:4.13626+0.00641949\n",
      "[100]\ttrain-error:59.9489+0.896892\ttrain-rmse:2.6363+0.000452564\ttest-error:61.021+6.85127\ttest-rmse:2.63631+0.00597248\n",
      "[200]\ttrain-error:46.0227+0.687006\ttrain-rmse:1.68094+0.000293649\ttest-error:46.843+5.24342\ttest-rmse:1.68094+0.00521488\n",
      "[300]\ttrain-error:29.9328+0.445221\ttrain-rmse:1.07269+0.000198254\ttest-error:30.4628+3.38545\ttest-rmse:1.07267+0.00489446\n",
      "[400]\ttrain-error:16.7851+0.248649\ttrain-rmse:0.685869+0.000161564\ttest-error:17.0804+1.8777\ttest-rmse:0.685887+0.00491839\n",
      "[500]\ttrain-error:8.31567+0.121778\ttrain-rmse:0.440581+0.000169186\ttest-error:8.4611+0.920751\ttest-rmse:0.440608+0.00510403\n",
      "[600]\ttrain-error:3.65581+0.0526242\ttrain-rmse:0.286052+0.000218876\ttest-error:3.7224+0.411158\ttest-rmse:0.286156+0.00543302\n",
      "[700]\ttrain-error:1.34116+0.0195735\ttrain-rmse:0.190059+0.000318611\ttest-error:1.3743+0.179001\ttest-rmse:0.190399+0.00592835\n",
      "[800]\ttrain-error:0.258962+0.00810625\ttrain-rmse:0.132082+0.000441207\ttest-error:0.284964+0.103496\ttest-rmse:0.133011+0.00660494\n",
      "[900]\ttrain-error:-0.231851+0.00857145\ttrain-rmse:0.0987645+0.000576606\ttest-error:-0.200951+0.0927664\ttest-rmse:0.10065+0.00720088\n",
      "[1000]\ttrain-error:-0.453166+0.00982864\ttrain-rmse:0.0808932+0.000678974\ttest-error:-0.412694+0.092943\ttest-rmse:0.0839831+0.00753015\n",
      "[1100]\ttrain-error:-0.554655+0.0103886\ttrain-rmse:0.0719265+0.000748834\ttest-error:-0.503895+0.0927112\ttest-rmse:0.0761966+0.00759324\n",
      "[1200]\ttrain-error:-0.603886+0.0107318\ttrain-rmse:0.0674974+0.0008042\ttest-error:-0.542886+0.0918918\ttest-rmse:0.0728464+0.00753121\n",
      "[1300]\ttrain-error:-0.630755+0.0105093\ttrain-rmse:0.0651504+0.000808734\ttest-error:-0.559612+0.0910734\ttest-rmse:0.0714802+0.00744351\n",
      "[1400]\ttrain-error:-0.647216+0.0103099\ttrain-rmse:0.0637594+0.000814374\ttest-error:-0.566692+0.0905262\ttest-rmse:0.0709616+0.00738212\n",
      "[1500]\ttrain-error:-0.659098+0.0099157\ttrain-rmse:0.0627759+0.000796205\ttest-error:-0.569457+0.0901775\ttest-rmse:0.070799+0.00733892\n",
      "('best num_boost_rounds = ', 1573)\n"
     ]
    }
   ],
   "source": [
    "y_train = train['y'].values\n",
    "y_mean = np.mean(y_train)\n",
    "id_test = test['ID'].values\n",
    "#finaltrainset and finaltestset are data to be used only the stacked model (does not contain PCA, SVD... arrays) \n",
    "finaltrainset = train[usable_columns].values\n",
    "finaltestset = test[usable_columns].values\n",
    "\n",
    "x_train = np.array(train.drop('y', axis=1))\n",
    "x_test = np.array(test)\n",
    "'''Train the xgb model then predict the test data'''\n",
    "print x_train.shape\n",
    "print x_test.shape\n",
    "xgb_params = {\n",
    "#     'n_trees': 520, \n",
    "    'eta': 0.0045,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.93,\n",
    "    'colsample_by_tree' : 0.65,\n",
    "    'objective': 'reg:linear',\n",
    "#     'gamma' : np.log(2),\n",
    "#     'min_child_weight' : np.log(10),\n",
    "#     'reg_alpha' : np.log(2),\n",
    "#     'eval_metric': 'rmse',\n",
    "#     'base_score': y_mean, # base prediction = mean(target)\n",
    "#     'silent': 1,\n",
    "    'seed' : 10,\n",
    "}\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import log_loss,mean_squared_error,r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def r2_eval(y, y0):\n",
    "    y0=y0.get_label()    \n",
    "    assert len(y) == len(y0)\n",
    "    return 'error',-(r2_score((y0),(y)))\n",
    "\n",
    "# NOTE: Make sure that the class is labeled 'class' in the data file\n",
    "\n",
    "dtrain = xgb.DMatrix(train.drop('y', axis=1), y_train)\n",
    "dtest = xgb.DMatrix(test)\n",
    "\n",
    "cv_output = xgb.cv(dict(xgb_params), dtrain, num_boost_round=5000,nfold = 10, feval = r2_eval,\n",
    "                   early_stopping_rounds=25,verbose_eval=100, show_stdv=True)\n",
    "print('best num_boost_rounds = ', len(cv_output))\n",
    "num_boost_rounds = len(cv_output) \n",
    "# num_boost_rounds = 1250\n",
    "# # train model\n",
    "# model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\n",
    "# y_pred = model.predict(dtest)\n",
    "# [1600]\ttrain-error:-0.65493+0.00891481\ttrain-rmse:0.0581227+0.000641751\ttest-error:-0.572537+0.0903409\ttest-rmse:0.0648515+0.00689838"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4209, 517)\n",
      "(4209, 517)\n",
      "[0]\ttrain-rmse:100.539\tvalid-rmse:100.433\ttrain-error:60.8453\tvalid-error:66.2964\n",
      "Multiple eval metrics have been passed: 'valid-error' will be used for early stopping.\n",
      "\n",
      "Will train until valid-error hasn't improved in 50 rounds.\n",
      "[100]\ttrain-rmse:64.4495\tvalid-rmse:64.2335\ttrain-error:24.4145\tvalid-error:26.5275\n",
      "[200]\ttrain-rmse:41.6259\tvalid-rmse:41.2975\ttrain-error:9.60153\tvalid-error:10.3786\n",
      "[300]\ttrain-rmse:27.3388\tvalid-rmse:26.9009\ttrain-error:3.57298\tvalid-error:3.82812\n",
      "[400]\ttrain-rmse:18.5698\tvalid-rmse:18.0297\ttrain-error:1.10986\tvalid-error:1.1688\n",
      "[500]\ttrain-rmse:13.3804\tvalid-rmse:12.7877\ttrain-error:0.095422\tvalid-error:0.090999\n",
      "[600]\ttrain-rmse:10.4812\tvalid-rmse:9.9031\ttrain-error:-0.327852\tvalid-error:-0.345687\n",
      "[700]\ttrain-rmse:8.9774\tvalid-rmse:8.46419\ttrain-error:-0.506891\tvalid-error:-0.522015\n",
      "[800]\ttrain-rmse:8.21133\tvalid-rmse:7.8192\ttrain-error:-0.587458\tvalid-error:-0.592087\n",
      "[900]\ttrain-rmse:7.80886\tvalid-rmse:7.55221\ttrain-error:-0.626907\tvalid-error:-0.619469\n",
      "[1000]\ttrain-rmse:7.57249\tvalid-rmse:7.45441\ttrain-error:-0.649151\tvalid-error:-0.629261\n",
      "[1100]\ttrain-rmse:7.42059\tvalid-rmse:7.42178\ttrain-error:-0.663087\tvalid-error:-0.632499\n",
      "[1200]\ttrain-rmse:7.30869\tvalid-rmse:7.41611\ttrain-error:-0.673171\tvalid-error:-0.63306\n",
      "Stopping. Best iteration:\n",
      "[1203]\ttrain-rmse:7.30646\tvalid-rmse:7.41566\ttrain-error:-0.67337\tvalid-error:-0.633104\n",
      "\n",
      "[0.63310415552712973]\n",
      "[0]\ttrain-rmse:100.639\tvalid-rmse:100.03\ttrain-error:61.8864\tvalid-error:61.8795\n",
      "Multiple eval metrics have been passed: 'valid-error' will be used for early stopping.\n",
      "\n",
      "Will train until valid-error hasn't improved in 50 rounds.\n",
      "[100]\ttrain-rmse:64.5006\tvalid-rmse:63.9383\ttrain-error:24.8317\tvalid-error:24.6904\n",
      "[200]\ttrain-rmse:41.6414\tvalid-rmse:41.0993\ttrain-error:9.76653\tvalid-error:9.61495\n",
      "[300]\ttrain-rmse:27.3251\tvalid-rmse:26.8081\ttrain-error:3.63605\tvalid-error:3.51628\n",
      "[400]\ttrain-rmse:18.5325\tvalid-rmse:18.0755\ttrain-error:1.13253\tvalid-error:1.05319\n",
      "[500]\ttrain-rmse:13.3099\tvalid-rmse:12.9881\ttrain-error:0.099952\tvalid-error:0.060084\n",
      "[600]\ttrain-rmse:10.3957\tvalid-rmse:10.2596\ttrain-error:-0.328985\tvalid-error:-0.33853\n",
      "[700]\ttrain-rmse:8.87617\tvalid-rmse:8.93656\ttrain-error:-0.510812\tvalid-error:-0.498131\n",
      "[800]\ttrain-rmse:8.11254\tvalid-rmse:8.35466\ttrain-error:-0.591362\tvalid-error:-0.561361\n",
      "[900]\ttrain-rmse:7.71104\tvalid-rmse:8.12162\ttrain-error:-0.630809\tvalid-error:-0.58549\n",
      "[1000]\ttrain-rmse:7.47585\tvalid-rmse:8.03097\ttrain-error:-0.652987\tvalid-error:-0.594691\n",
      "[1100]\ttrain-rmse:7.32747\tvalid-rmse:8.00441\ttrain-error:-0.666625\tvalid-error:-0.597368\n",
      "[1200]\ttrain-rmse:7.20906\tvalid-rmse:8.00083\ttrain-error:-0.677312\tvalid-error:-0.597728\n",
      "Stopping. Best iteration:\n",
      "[1186]\ttrain-rmse:7.22426\tvalid-rmse:8.00001\ttrain-error:-0.67595\tvalid-error:-0.597811\n",
      "\n",
      "[0.63310415552712973, 0.59781091047231094]\n",
      "[0]\ttrain-rmse:100.607\tvalid-rmse:100.16\ttrain-error:61.1304\tvalid-error:65.0546\n",
      "Multiple eval metrics have been passed: 'valid-error' will be used for early stopping.\n",
      "\n",
      "Will train until valid-error hasn't improved in 50 rounds.\n",
      "[100]\ttrain-rmse:64.491\tvalid-rmse:64.0686\ttrain-error:24.5298\tvalid-error:26.0273\n",
      "[200]\ttrain-rmse:41.6518\tvalid-rmse:41.2012\ttrain-error:9.64918\tvalid-error:10.1772\n",
      "[300]\ttrain-rmse:27.3485\tvalid-rmse:26.833\ttrain-error:3.59109\tvalid-error:3.74079\n",
      "[400]\ttrain-rmse:18.5702\tvalid-rmse:17.9898\ttrain-error:1.1168\tvalid-error:1.1309\n",
      "[500]\ttrain-rmse:13.3755\tvalid-rmse:12.7612\ttrain-error:0.098167\tvalid-error:0.072249\n",
      "[600]\ttrain-rmse:10.4681\tvalid-rmse:9.90132\ttrain-error:-0.327351\tvalid-error:-0.354496\n",
      "[700]\ttrain-rmse:8.94975\tvalid-rmse:8.48909\ttrain-error:-0.508333\tvalid-error:-0.525501\n",
      "[800]\ttrain-rmse:8.18104\tvalid-rmse:7.87176\ttrain-error:-0.589166\tvalid-error:-0.592003\n",
      "[900]\ttrain-rmse:7.77869\tvalid-rmse:7.62139\ttrain-error:-0.628583\tvalid-error:-0.617544\n",
      "[1000]\ttrain-rmse:7.54153\tvalid-rmse:7.52645\ttrain-error:-0.650886\tvalid-error:-0.627014\n",
      "[1100]\ttrain-rmse:7.38262\tvalid-rmse:7.50093\ttrain-error:-0.665443\tvalid-error:-0.629539\n",
      "[1200]\ttrain-rmse:7.26524\tvalid-rmse:7.50138\ttrain-error:-0.675997\tvalid-error:-0.629494\n",
      "Stopping. Best iteration:\n",
      "[1156]\ttrain-rmse:7.3151\tvalid-rmse:7.49752\ttrain-error:-0.671534\tvalid-error:-0.629876\n",
      "\n",
      "[0.63310415552712973, 0.59781091047231094, 0.62987554580494609]\n",
      "[0]\ttrain-rmse:100.513\tvalid-rmse:100.54\ttrain-error:60.2683\tvalid-error:69.1679\n",
      "Multiple eval metrics have been passed: 'valid-error' will be used for early stopping.\n",
      "\n",
      "Will train until valid-error hasn't improved in 50 rounds.\n",
      "[100]\ttrain-rmse:64.4227\tvalid-rmse:64.4395\ttrain-error:24.1694\tvalid-error:27.8249\n",
      "[200]\ttrain-rmse:41.5922\tvalid-rmse:41.5972\ttrain-error:9.49105\tvalid-error:11.0114\n",
      "[300]\ttrain-rmse:27.2928\tvalid-rmse:27.2879\ttrain-error:3.51741\tvalid-error:4.16899\n",
      "[400]\ttrain-rmse:18.5069\tvalid-rmse:18.508\ttrain-error:1.07712\tvalid-error:1.37784\n",
      "[500]\ttrain-rmse:13.3061\tvalid-rmse:13.3598\ttrain-error:0.073739\tvalid-error:0.238985\n",
      "[600]\ttrain-rmse:10.3892\tvalid-rmse:10.552\ttrain-error:-0.345431\tvalid-error:-0.227077\n",
      "[700]\ttrain-rmse:8.87575\tvalid-rmse:9.14513\ttrain-error:-0.522245\tvalid-error:-0.419444\n",
      "[800]\ttrain-rmse:8.09966\tvalid-rmse:8.49563\ttrain-error:-0.602142\tvalid-error:-0.498979\n",
      "[900]\ttrain-rmse:7.69989\tvalid-rmse:8.2111\ttrain-error:-0.640446\tvalid-error:-0.531977\n",
      "[1000]\ttrain-rmse:7.45228\tvalid-rmse:8.08596\ttrain-error:-0.663199\tvalid-error:-0.546133\n",
      "[1100]\ttrain-rmse:7.30194\tvalid-rmse:8.03482\ttrain-error:-0.676651\tvalid-error:-0.551856\n",
      "[1200]\ttrain-rmse:7.17648\tvalid-rmse:8.01098\ttrain-error:-0.687667\tvalid-error:-0.554512\n",
      "[1300]\ttrain-rmse:7.06764\tvalid-rmse:8.00493\ttrain-error:-0.697069\tvalid-error:-0.555185\n",
      "[1400]\ttrain-rmse:6.97753\tvalid-rmse:8.00147\ttrain-error:-0.704744\tvalid-error:-0.555569\n",
      "Stopping. Best iteration:\n",
      "[1386]\ttrain-rmse:6.98862\tvalid-rmse:8.00061\ttrain-error:-0.703805\tvalid-error:-0.555665\n",
      "\n",
      "[0.63310415552712973, 0.59781091047231094, 0.62987554580494609, 0.55566451968699759]\n",
      "[0]\ttrain-rmse:100.292\tvalid-rmse:101.423\ttrain-error:65.4848\tvalid-error:50.9874\n",
      "Multiple eval metrics have been passed: 'valid-error' will be used for early stopping.\n",
      "\n",
      "Will train until valid-error hasn't improved in 50 rounds.\n",
      "[100]\ttrain-rmse:64.2271\tvalid-rmse:65.4372\ttrain-error:26.2663\tvalid-error:20.641\n",
      "[200]\ttrain-rmse:41.3828\tvalid-rmse:42.7557\ttrain-error:10.3196\tvalid-error:8.23879\n",
      "[300]\ttrain-rmse:27.0426\tvalid-rmse:28.6898\ttrain-error:3.83377\tvalid-error:3.1599\n",
      "[400]\ttrain-rmse:18.1904\tvalid-rmse:20.2342\ttrain-error:1.18712\tvalid-error:1.06918\n",
      "[500]\ttrain-rmse:12.8974\tvalid-rmse:15.4294\ttrain-error:0.099488\tvalid-error:0.203174\n",
      "[600]\ttrain-rmse:9.90667\tvalid-rmse:12.895\ttrain-error:-0.3513\tvalid-error:-0.159635\n",
      "[700]\ttrain-rmse:8.33237\tvalid-rmse:11.6672\ttrain-error:-0.541092\tvalid-error:-0.312046\n",
      "[800]\ttrain-rmse:7.53665\tvalid-rmse:11.104\ttrain-error:-0.624555\tvalid-error:-0.376863\n",
      "[900]\ttrain-rmse:7.13191\tvalid-rmse:10.8542\ttrain-error:-0.663797\tvalid-error:-0.40458\n",
      "[1000]\ttrain-rmse:6.90637\tvalid-rmse:10.746\ttrain-error:-0.684725\tvalid-error:-0.416387\n",
      "[1100]\ttrain-rmse:6.75733\tvalid-rmse:10.6974\ttrain-error:-0.698186\tvalid-error:-0.421656\n",
      "[1200]\ttrain-rmse:6.65216\tvalid-rmse:10.6786\ttrain-error:-0.707508\tvalid-error:-0.423691\n",
      "[1300]\ttrain-rmse:6.56575\tvalid-rmse:10.673\ttrain-error:-0.715058\tvalid-error:-0.42429\n",
      "[1400]\ttrain-rmse:6.48522\tvalid-rmse:10.6737\ttrain-error:-0.722004\tvalid-error:-0.424223\n",
      "Stopping. Best iteration:\n",
      "[1364]\ttrain-rmse:6.51456\tvalid-rmse:10.6713\ttrain-error:-0.719483\tvalid-error:-0.424481\n",
      "\n",
      "[0.63310415552712973, 0.59781091047231094, 0.62987554580494609, 0.55566451968699759, 0.42448092020632788]\n"
     ]
    }
   ],
   "source": [
    "y_train = train['y'].values\n",
    "y_mean = np.mean(y_train)\n",
    "id_test = test['ID'].values\n",
    "#finaltrainset and finaltestset are data to be used only the stacked model (does not contain PCA, SVD... arrays) \n",
    "finaltrainset = train[usable_columns].values\n",
    "finaltestset = test[usable_columns].values\n",
    "\n",
    "x_train = np.array(train.drop('y', axis=1))\n",
    "x_test = np.array(test)\n",
    "'''Train the xgb model then predict the test data'''\n",
    "print x_train.shape\n",
    "print x_test.shape\n",
    "# xgb_params = {\n",
    "# #     'n_trees': 520, \n",
    "#     'eta': 0.005,\n",
    "#     'max_depth': 6,\n",
    "#     'subsample': 0.95,\n",
    "#     'colsample_by_tree'\n",
    "#     'objective': 'reg:linear',\n",
    "#     'eval_metric': 'rmse',\n",
    "#     'base_score': y_mean, # base prediction = mean(target)\n",
    "#     'silent': 1,\n",
    "# #     'seed' : 2016,\n",
    "# }\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import log_loss,mean_squared_error,r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def r2_eval(y, y0):\n",
    "    y0=y0.get_label()    \n",
    "    assert len(y) == len(y0)\n",
    "    return 'error',-(r2_score(((y0)),(y)))\n",
    "\n",
    "\n",
    "# Set our parameters for xgboost\n",
    "params = {}\n",
    "params['objective'] = 'reg:linear'\n",
    "# params['n_trees'] = 520\n",
    "# params['eval_metric'] = 'rmse'\n",
    "params['eta'] = 0.0045\n",
    "params['max_depth'] = 4\n",
    "params['seed'] = 420\n",
    "# params['gamma'] = 2\n",
    "params['subsample'] = 0.93\n",
    "params['colsample_bytree'] = 0.4\n",
    "# params['base_score'] = y_mean\n",
    "# params['min_child_weight'] = 10\n",
    "# params['reg_alpha'] = 2\n",
    "# params['reg_lambda'] = 2\n",
    "params['n_jobs'] = 32\n",
    "\n",
    "# NOTE: Make sure that the class is labeled 'class' in the data file\n",
    "\n",
    "# dtrain = xgb.DMatrix(train.drop('y', axis=1), y_train)\n",
    "# dtest = xgb.DMatrix(test)\n",
    "\n",
    "# num_boost_rounds = 1250\n",
    "# # train model\n",
    "# model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\n",
    "# y_pred = model.predict(dtest)\n",
    "\n",
    "\n",
    "# y_train = np.array(y_train)\n",
    "\n",
    "train_stacker=[ [0.0 for s in range(1)]  for k in range (0,(x_train.shape[0])) ]\n",
    "\n",
    "cv_scores = []\n",
    "oof_preds = []\n",
    "a = [0 for x in range(0,x_test.shape[0])]\n",
    "# StratifiedKFold\n",
    "# kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2016)\n",
    "# for dev_index, val_index in kf.split(range(x_train.shape[0]),y_train):\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=420)\n",
    "for dev_index, val_index in kf.split(range(x_train.shape[0])):\n",
    "        dev_X, val_X = x_train[dev_index,:], x_train[val_index,:]\n",
    "        dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "        d_train = xgb.DMatrix(dev_X, label=dev_y)\n",
    "        d_valid = xgb.DMatrix(val_X, label=val_y)\n",
    "\n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "        bst = xgb.train(params, d_train, 5000, watchlist, feval = r2_eval, early_stopping_rounds=50, verbose_eval=100)\n",
    "        # ntree_limit=model.best_ntree_limit\n",
    "        preds = bst.predict(d_valid, ntree_limit=bst.best_ntree_limit)\n",
    "\n",
    "        cv_scores.append((r2_score(val_y  , preds)))\n",
    "\n",
    "        print(cv_scores)\n",
    "#         break\n",
    "        \n",
    "        d_test = xgb.DMatrix(x_test)\n",
    "        preds_tr = bst.predict(d_test, ntree_limit=bst.best_ntree_limit)\n",
    "        \n",
    "        a = np.column_stack((a,preds_tr))\n",
    "\n",
    "        no=0\n",
    "        for real_index in val_index:\n",
    "            for d in range (0,1):\n",
    "                train_stacker[real_index][d]=(preds[no])\n",
    "            no+=1\n",
    "\n",
    "# [0.6083777650601262, 0.54277680892907387, 0.61289682196068918, 0.47706217814929963, 0.61092343331253285]\n",
    "\n",
    "# [0.63310415552712973, 0.59781091047231094, 0.62987554580494609, 0.55566451968699759, 0.42448092020632788]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = np.exp(train_stacker) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = pd.DataFrame(a)\n",
    "\n",
    "b['sum'] = b.sum(axis = 1)/5\n",
    "\n",
    "np.savetxt(\"xg_1_train.gz\",train_stacker, delimiter=\",\", fmt='%.6f')\n",
    "\n",
    "np.savetxt(\"xg_1_test.gz\", b['sum'], delimiter=\",\", fmt='%.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        84.956982\n",
       "1        96.713034\n",
       "2        84.396162\n",
       "3        84.077484\n",
       "4       115.538089\n",
       "5        93.197830\n",
       "6       113.588580\n",
       "7        95.214503\n",
       "8       119.768359\n",
       "9        97.151657\n",
       "10      118.995477\n",
       "11      106.476736\n",
       "12       99.437073\n",
       "13       95.174823\n",
       "14      105.545950\n",
       "15      101.190536\n",
       "16      117.877576\n",
       "17       99.337416\n",
       "18       95.250378\n",
       "19       94.297211\n",
       "20       96.819792\n",
       "21       95.831041\n",
       "22       93.066489\n",
       "23       96.526428\n",
       "24       93.223253\n",
       "25      116.718222\n",
       "26      103.982430\n",
       "27      104.472185\n",
       "28       93.900458\n",
       "29       84.755946\n",
       "           ...    \n",
       "4179    109.617030\n",
       "4180    101.696504\n",
       "4181     92.450688\n",
       "4182     92.347440\n",
       "4183    101.663240\n",
       "4184    110.027531\n",
       "4185     92.144193\n",
       "4186     94.257620\n",
       "4187    111.145135\n",
       "4188    109.264182\n",
       "4189     91.539748\n",
       "4190    109.825067\n",
       "4191     90.967030\n",
       "4192    101.271576\n",
       "4193     92.287230\n",
       "4194    111.092201\n",
       "4195     92.744778\n",
       "4196    102.462994\n",
       "4197    101.287067\n",
       "4198    110.733488\n",
       "4199     91.757121\n",
       "4200     90.569096\n",
       "4201     93.652139\n",
       "4202    109.728702\n",
       "4203    107.372292\n",
       "4204    102.409319\n",
       "4205     93.313309\n",
       "4206     93.215117\n",
       "4207    109.794189\n",
       "4208     92.602684\n",
       "Name: sum, Length: 4209, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b['sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = pd.DataFrame(a)\n",
    "b['sum'] = b.sum(axis = 1)/5\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = b['sum']\n",
    "sub.to_csv('single_xgb6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y1 = [0 for x in range(x_train.shape[0])]\n",
    "y2 = [1 for x in range(x_test.shape[0])]\n",
    "\n",
    "yn = np.column_stack((y1,y2))\n",
    "print yn.shape\n",
    "\n",
    "y_train = np.column_stack((np.log(train['y'].values + 1)), np.log(b['sum'] + 1))\n",
    "y_mean = np.mean(y_train)\n",
    "id_test = test['ID'].values\n",
    "#finaltrainset and finaltestset are data to be used only the stacked model (does not contain PCA, SVD... arrays) \n",
    "finaltrainset = train[usable_columns].values\n",
    "finaltestset = test[usable_columns].values\n",
    "\n",
    "x_train = np.array(train.drop('y', axis=1))\n",
    "x_test = np.array(test)\n",
    "X_train = np.column_stack((x_train,x_test))\n",
    "\n",
    "'''Train the xgb model then predict the test data'''\n",
    "print X_train.shape\n",
    "print x_test.shape\n",
    "\n",
    "train_stacker=[ [0.0 for s in range(1)]  for k in range (0,(X_train.shape[0])) ]\n",
    "\n",
    "cv_scores = []\n",
    "oof_preds = []\n",
    "a = [0 for x in range(0,x_test.shape[0])]\n",
    "# StratifiedKFold\n",
    "# kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=RS)\n",
    "# for dev_index, val_index in kf.split(range(x_train.shape[0]),y_train):\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=420)\n",
    "for dev_index, val_index in kf.split(range(X_train.shape[0])):\n",
    "        dev_X, val_X = X_train[dev_index,:], X_train[val_index,:]\n",
    "        dev_y, val_y, dev_yn, val_yn = y_train[dev_index], y_train[val_index], yn[dev_index], yn[val_index] \n",
    "        print dev_X.shape\n",
    "        print val_X.shape\n",
    "        \n",
    "        pos_train = dev_X[dev_yn == 1]\n",
    "        neg_train = dev_X[dev_yn == 0]\n",
    "\n",
    "        print(\"Oversampling started for proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
    "        p = 0.25\n",
    "#         scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "#         while scale > 1:\n",
    "#             neg_train = np.concatenate((neg_train, neg_train))\n",
    "#             scale -=1\n",
    "        pos_train = pos_train[:int(scale * len(pos_train))]\n",
    "        print(\"Oversampling done, new proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
    "\n",
    "        Xd = np.concatenate((pos_train, neg_train))\n",
    "        yd = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "        del pos_train, neg_train  \n",
    "\n",
    "        pos_train = val_X[val_y == 1]\n",
    "        neg_train = val_X[val_y == 0]\n",
    "\n",
    "        print(\"Oversampling started for proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
    "        p = 0.165\n",
    "        scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "        while scale > 1:\n",
    "            neg_train = np.concatenate((neg_train, neg_train))\n",
    "            scale -=1\n",
    "        neg_train = np.concatenate((neg_train, neg_train[:int(scale * len(neg_train))]))\n",
    "        print(\"Oversampling done, new proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
    "\n",
    "        Xv = np.concatenate((pos_train, neg_train))\n",
    "        yv = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "        del pos_train, neg_train  \n",
    "\n",
    "        print dev_X.shape\n",
    "        print val_X.shape\n",
    "\n",
    "        d_train = xgb.DMatrix(Xd, label=yd)\n",
    "        d_valid = xgb.DMatrix(Xv, label=yv)\n",
    "\n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "        bst = xgb.train(params, d_train, 5000, watchlist, early_stopping_rounds=25, verbose_eval=100)\n",
    "        # ntree_limit=model.best_ntree_limit\n",
    "        preds = bst.predict(d_valid, ntree_limit=bst.best_ntree_limit)\n",
    "        cv_scores.append(log_loss(yv, preds))\n",
    "        print(cv_scores)\n",
    "#         break\n",
    "        \n",
    "        d_test = xgb.DMatrix(x_test)\n",
    "        preds_tr = bst.predict(d_test, ntree_limit=bst.best_ntree_limit)\n",
    "\n",
    "        a = np.column_stack((a,preds_tr))\n",
    "\n",
    "        d_valorg = xgb.DMatrix(val_X, label=val_y)\n",
    "        predsorg = bst.predict(d_valorg, ntree_limit=bst.best_ntree_limit)\n",
    "\n",
    "#         predictions = preds.reshape(-1,1)\n",
    "        no=0\n",
    "        for real_index in val_index:\n",
    "            for d in range (0,1):\n",
    "                train_stacker[real_index][d]=(predsorg[no])\n",
    "            no+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Train the stacked models then predict the test data'''\n",
    "\n",
    "stacked_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=LassoLarsCV(normalize=True)),\n",
    "    StackingEstimator(estimator=GradientBoostingRegressor(learning_rate=0.001, loss=\"huber\", max_depth=3, max_features=0.55, min_samples_leaf=18, min_samples_split=14, subsample=0.7)),\n",
    "    LassoLarsCV()\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "stacked_pipeline.fit(finaltrainset, y_train)\n",
    "results = stacked_pipeline.predict(finaltestset)\n",
    "\n",
    "'''R2 Score on the entire Train data when averaging'''\n",
    "\n",
    "print('R2 score on train data:')\n",
    "print(r2_score(y_train,stacked_pipeline.predict(finaltrainset)*0.2855 + model.predict(dtrain)*0.7145))\n",
    "\n",
    "'''Average the preditionon test data  of both models then save it on a csv file'''\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = y_pred*0.75 + results*0.25\n",
    "sub.to_csv('stacked-models3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score on train data:\n",
      "0.670214876937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Average the preditionon test data  of both models then save it on a csv file'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('R2 score on train data:')\n",
    "print(r2_score(y_train,stacked_pipeline.predict(finaltrainset)*0.25 + model.predict(dtrain)*0.75))\n",
    "\n",
    "'''Average the preditionon test data  of both models then save it on a csv file'''\n",
    "\n",
    "# sub = pd.DataFrame()\n",
    "# sub['ID'] = id_test\n",
    "# sub['y'] = y_pred*0.71455 + results*0.2855\n",
    "# sub.to_csv('stacked-models3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
