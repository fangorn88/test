{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your password:········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/sqlalchemy/dialects/oracle/cx_oracle.py:753: SAWarning: cx_Oracle is compiled under Python 2.xx using the WITH_UNICODE flag.  Consider recompiling cx_Oracle without this flag, which is in no way necessary for full support of Unicode. Otherwise, all string-holding bind parameters must be explicitly typed using SQLAlchemy's String type or one of its subtypes,or otherwise be passed as Python unicode.  Plain Python strings passed as bind parameters will be silently corrupted by cx_Oracle.\n",
      "  \"cx_Oracle is compiled under Python 2.xx using the \"\n"
     ]
    }
   ],
   "source": [
    "# # Importing relevant packages and establishing the Exadata connection\n",
    "\n",
    "# import sqlalchemy\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import getpass\n",
    "\n",
    "# password=getpass.getpass(\"Enter your password:\")\n",
    "# engine = sqlalchemy.create_engine(\"oracle+cx_oracle://ashwinik[an_rt_ws18]:%s@exa_gbtukprdd_an\" %(password))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xdatadump userid='ashwinik[an_rt_ws18]/Welcome16@exa_gbtukprdd_an' sqlstmt='SELECT  * FROM  AK_final_test ' delimiter=^ header=y > exp_trans_13wk.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "## Using xdatadump to get transactions from exadata table\n",
    "\n",
    "# import subprocess\n",
    "# import os\n",
    "# from urllib import quote_plus as urlquote\n",
    "\n",
    "# # DROP TABLE AK_EXP_TRANS;\n",
    "# # CREATE TABLE AK_EXP_TRANS PARALLEL COMPRESS FOR QUERY HIGH AS \n",
    "# # SELECT TRANSACTION_FID,CONVERTED_PROD_GROUP_ID AS TPNB, 1 AS VALUE\n",
    "# # FROM SMP10_TRAN_ITEM_FCT A \n",
    "# # INNER JOIN PROD_DIM_C B ON A.PROD_ID = B.PROD_ID\n",
    "# # INNER JOIN DATE_DIM D ON A.DATE_ID = D.DATE_ID\n",
    "# # INNER JOIN STORE_DIM_C C ON A.STORE_ID = C.STORE_ID \n",
    "# # WHERE PROD_MERCH_L20_CODE IN ('S1F','B1B','F1A','D2A','D2P','U2N','S3A','G3C','G1C','G1F','S3K','P3E','G1T')\n",
    "# # AND FIS_WEEK_ID BETWEEN 201649 AND 201652 AND FORMAT_CODE = 'X'; \n",
    "\n",
    "# proxy = \"ashwinik[an_rt_ws18]/%s@exa_gbtukprdd_an\" %(password)\n",
    "\n",
    "# cmd_str =   \"xdatadump userid='\" + str(proxy)                 + \"' sqlstmt='SELECT  * FROM  AK_final_test \"                 +  \"' delimiter=^ header=y > exp_trans_13wk.csv\"        \n",
    "\n",
    "# print cmd_str\n",
    "# subprocess.call(cmd_str, cwd='./', shell=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1742404, 3687)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# # In[3]:\n",
    "\n",
    "\n",
    "# # Importing the transaction file to be used to create the adjacency matrix\n",
    "\n",
    "# # tp = pd.read_csv('exp_trans_13wk.csv',header=0, delimiter=\"^\",chunksize = 10000, iterator=False)\n",
    "# # df = pd.concat(tp, ignore_index=True)\n",
    "\n",
    "# df1 = pd.read_csv('exp_trans_13wk.csv',header=0, delimiter=\"^\").groupby(['TRANSACTION_FID','TPNB'])['VALUE'].max().unstack()\n",
    "\n",
    "\n",
    "\n",
    "# # In[4]:\n",
    "\n",
    "\n",
    "# df1.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # In[5]:\n",
    "\n",
    "\n",
    "# # print df1.head()\n",
    "# # print df1.describe()\n",
    "\n",
    "# # df1\n",
    "# # df1.fillna(0,inplace = True)\n",
    "# X = np.array(df1)\n",
    "# X[~np.isfinite(X)]= 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model for 1000 components\n",
      "violation: 1.0\n",
      "violation: 0.0107537182211\n",
      "violation: 0.000613995865981\n",
      "violation: 3.49011101596e-05\n",
      "Converged at iteration 4\n",
      "done in 10.970s.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "import time\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from scipy import sparse\n",
    "\n",
    "from scipy import sparse, io\n",
    "\n",
    "X_sparse = io.mmread(\"test.mtx\")\n",
    "\n",
    "# X = df1.as_matrix()\n",
    "# X_sparse = sparse.csr_matrix(X)\n",
    "\n",
    "print \"Fitting the NMF model for 1000 components\"  \n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "nmf = NMF(n_components=1000, random_state=1,\n",
    "          alpha= 0,verbose = 1, l1_ratio= 0,init = 'nndsvd')\n",
    "nmf.fit(X_sparse)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time.time() - t0))\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "with open('nmf.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(nmf, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
