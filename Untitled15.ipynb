{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "import glob\n",
    "import cv2\n",
    "import math\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, \\\n",
    "                                       ZeroPadding2D\n",
    "\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "# from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json\n",
    "# from sklearn.metrics import log_loss\n",
    "from numpy.random import permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2016)\n",
    "use_cache = 1\n",
    "# color type: 1 - grey, 3 - rgb\n",
    "color_type_global = 3\n",
    "img_rows = 64\n",
    "img_cols = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_im(path, img_rows, img_cols, color_type=3):\n",
    "    # Load as color\n",
    "    if color_type == 1:\n",
    "        img = cv2.imread(path, 0)\n",
    "    elif color_type == 3:\n",
    "        img = cv2.imread(path)\n",
    "    # Reduce size\n",
    "    resized = cv2.resize(img, (img_cols, img_rows),cv2.INTER_LINEAR)\n",
    "    # mean_pixel = [103.939, 116.799, 123.68]\n",
    "    # resized = resized.astype(np.float32, copy=False)\n",
    "\n",
    "    # for c in range(3):\n",
    "    #    resized[:, :, c] = resized[:, :, c] - mean_pixel[c]\n",
    "    # resized = resized.transpose((2, 0, 1))\n",
    "    # resized = np.expand_dims(img, axis=0)\n",
    "    return resized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "encoded = le.fit(['LAG', 'YFT', 'OTHER', 'DOL', 'SHARK', 'NoF', 'BET', 'ALB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def load_train(img_rows=64, img_cols=64, color_type=3):    \n",
    "#     X_train = []\n",
    "#     y_train = []\n",
    "#     print('Read train images')\n",
    "#     folders = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "#     for j in folders:\n",
    "#         index = folders.index(j)\n",
    "#         print('Load folder {}'.format(j))\n",
    "#         path = os.path.join('..', 'input', 'train', j , '*.jpg')\n",
    "#         files = glob.glob(path)\n",
    "#         for fl in files:\n",
    "#             flbase = os.path.basename(fl)\n",
    "#             img = get_im(fl, img_rows, img_cols, color_type)\n",
    "#             X_train.append(img)\n",
    "#             y_train.append(index)\n",
    "# #     y_train = le.transform(y_train)\n",
    "# #     y_train = LabelEncoder().fit_transform(y_train)\n",
    "# #     y_train = np_utils.to_categorical(y_train)\n",
    "#     return X_train, y_train\n",
    "\n",
    "def get_im_cv2(path):\n",
    "    img = cv2.imread(path)\n",
    "    resized = cv2.resize(img, (64, 64), cv2.INTER_LINEAR)\n",
    "    return resized\n",
    "\n",
    "def load_train():\n",
    "    X_train = []\n",
    "    X_train_id = []\n",
    "    y_train = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    print('Read train images')\n",
    "    folders = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "    for fld in folders:\n",
    "        index = folders.index(fld)\n",
    "        print('Load folder {} (Index: {})'.format(fld, index))\n",
    "        path = os.path.join('/nfs/science/shared/ipythonNotebooks/anantk/Kgl/Fish/train/', 'train', fld, '*.jpg')\n",
    "        files = glob.glob(path)\n",
    "        for fl in files:\n",
    "            flbase = os.path.basename(fl)\n",
    "            img = get_im_cv2(fl)\n",
    "            X_train.append(img)\n",
    "            X_train_id.append(flbase)\n",
    "            y_train.append(index)\n",
    "\n",
    "    print('Read train data time: {} seconds'.format(round(time.time() - start_time, 2)))\n",
    "    return X_train, y_train, X_train_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cache_data(data, path):\n",
    "    if not os.path.isdir('cache'):\n",
    "        os.mkdir('cache')\n",
    "    if os.path.isdir(os.path.dirname(path)):\n",
    "        file = open(path, 'wb')\n",
    "        pickle.dump(data, file)\n",
    "        file.close()\n",
    "    else:\n",
    "        print('Directory doesnt exists')\n",
    "        \n",
    "def restore_data(path):\n",
    "    data = dict()\n",
    "    if os.path.isfile(path):\n",
    "        print('Restore data from pickle....')\n",
    "        file = open(path, 'rb')\n",
    "        data = pickle.load(file)\n",
    "    return data\n",
    "\n",
    "def save_model(model, index, cross=''):\n",
    "    json_string = model.to_json()\n",
    "    if not os.path.isdir('cache'):\n",
    "        os.mkdir('cache')\n",
    "    json_name = 'architecture' + str(index) + cross + '.json'\n",
    "    weight_name = 'model_weights' + str(index) + cross + '.h5'\n",
    "    open(os.path.join('cache', json_name), 'w').write(json_string)\n",
    "    model.save_weights(os.path.join('cache', weight_name), overwrite=True)\n",
    "\n",
    "\n",
    "def read_model(index, cross=''):\n",
    "    json_name = 'architecture' + str(index) + cross + '.json'\n",
    "    weight_name = 'model_weights' + str(index) + cross + '.h5'\n",
    "    model = model_from_json(open(os.path.join('cache', json_name)).read())\n",
    "    model.load_weights(os.path.join('cache', weight_name))\n",
    "    return model\n",
    "\n",
    "\n",
    "def split_validation_set(train, target, test_size):\n",
    "    random_state = 1234\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(train, target,\n",
    "                         test_size=test_size,\n",
    "                         random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def dict_to_list(d):\n",
    "    ret = []\n",
    "    for i in d.items():\n",
    "        ret.append(i[1])\n",
    "    return ret\n",
    "\n",
    "\n",
    "def merge_several_folds_mean(data, nfolds):\n",
    "    a = np.array(data[0])\n",
    "    for i in range(1, nfolds):\n",
    "        a += np.array(data[i])\n",
    "    a /= nfolds\n",
    "    return a.tolist()\n",
    "\n",
    "\n",
    "def merge_several_folds_geom(data, nfolds):\n",
    "    a = np.array(data[0])\n",
    "    for i in range(1, nfolds):\n",
    "        a *= np.array(data[i])\n",
    "    a = np.power(a, 1/nfolds)\n",
    "    return a.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_test(img_rows, img_cols, color_type=1):\n",
    "    print('Read test images')\n",
    "    path = os.path.join('..', 'input','test_stg1', '*.jpg')\n",
    "    files = glob.glob(path)\n",
    "    X_test = []\n",
    "    X_test_id = []\n",
    "    total = 0\n",
    "    thr = math.floor(len(files)/100)\n",
    "    for fl in files:\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_im(fl, img_rows, img_cols, color_type)\n",
    "        X_test.append(img)\n",
    "        X_test_id.append(flbase)\n",
    "        total += 1\n",
    "        if total % thr == 0:\n",
    "            print('Read {} images from {}'.format(total, len(files)))\n",
    "\n",
    "    return X_test, X_test_id\n",
    "\n",
    "\n",
    "def create_submission(predictions, test_id, info):\n",
    "    result1 = pd.DataFrame(predictions, columns=['image', 'LAG', 'YFT', 'OTHER', 'DOL', 'SHARK', 'NoF', 'BET', 'ALB'])\n",
    "    result1.loc[:, 'image'] = pd.Series(test_id, index=result1.index)\n",
    "    now = datetime.datetime.now()\n",
    "    if not os.path.isdir('subm'):\n",
    "        os.mkdir('subm')\n",
    "    suffix = info + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\"))\n",
    "    sub_file = os.path.join('subm', 'submission_' + suffix + '.csv')\n",
    "    result1.to_csv(sub_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg_std16_model(img_rows, img_cols, color_type=3):\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=(color_type,\n",
    "                                                 img_rows, img_cols),dim_ordering='th'))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu',dim_ordering='th'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu',dim_ordering='th'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1),dim_ordering='th'))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu',dim_ordering='th'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu',dim_ordering='th'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1),dim_ordering='th'))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu',dim_ordering='th'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu',dim_ordering='th'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu',dim_ordering='th'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1),dim_ordering='th'))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu',dim_ordering='th'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu',dim_ordering='th'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu',dim_ordering='th'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1),dim_ordering='th'))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu',dim_ordering='th'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu',dim_ordering='th'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu',dim_ordering='th'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2),dim_ordering='th'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "    model.load_weights('/nfs/science/shared/ipythonNotebooks/anantk/vgg16.h5')\n",
    "\n",
    "    # Code above loads pre-trained data and\n",
    "    model.layers.pop()\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "    # Learning rate is changed to 0.001\n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "    print('model loading and compilation finished succesfully')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def read_and_normalize_and_shuffle_train_data(img_rows=64, img_cols=64,\n",
    "#                                               color_type=3):\n",
    "\n",
    "#     cache_path = os.path.join('cache', 'train_r_' + str(img_rows) +\n",
    "#                               '_c_' + str(img_cols) + '_t_' +\n",
    "#                               str(color_type) + '.dat')\n",
    "\n",
    "#     if not os.path.isfile(cache_path) or use_cache == 0:\n",
    "#         train_data, train_target = load_train(img_rows, img_cols, color_type)\n",
    "#         cache_data((train_data, train_target),cache_path)\n",
    "#     else:\n",
    "#         print('Restore train from cache!')\n",
    "#         (train_data, train_target) = restore_data(cache_path)\n",
    "# #     print('Convert to numpy...')\n",
    "# #     train_data = np.array(train_data, dtype=np.uint8)\n",
    "# #     train_target = np.array(train_target, dtype=np.uint8)\n",
    "        \n",
    "#     train_data = np.array(train_data, dtype=np.uint8)    \n",
    "#     train_target = np.array(train_target, dtype=np.uint8)\n",
    "\n",
    "#     train_data = train_data.transpose((0, 3, 1, 2))\n",
    "\n",
    "#     train_target = np_utils.to_categorical(train_target, 8)\n",
    "#     train_data = train_data.astype('float32')\n",
    "    \n",
    "#     ## check mean pixel value\n",
    "#     mean_pixel = [103.939, 116.779, 123.68]\n",
    "#     for c in range(3):\n",
    "#         train_data[:, c, :, :] = train_data[:, c, :, :] - mean_pixel[c]\n",
    "#     # train_data /= 255\n",
    "#     perm = permutation(len(train_target))\n",
    "#     train_data = train_data[perm]\n",
    "#     train_target = train_target[perm]\n",
    "#     print('Train shape:', train_data.shape)\n",
    "#     print(train_data.shape[0], 'train samples')\n",
    "#     return train_data, train_target\n",
    "\n",
    "\n",
    "# def read_and_normalize_test_data(img_rows=64, img_cols=64, color_type=3):\n",
    "#     cache_path = os.path.join('cache', 'test_r_' + str(img_rows) +\n",
    "#                               '_c_' + str(img_cols) + '_t_' +\n",
    "#                               str(color_type) + '.dat')\n",
    "#     if not os.path.isfile(cache_path) or use_cache == 0:\n",
    "#         test_data, test_id = load_test(img_rows, img_cols, color_type)\n",
    "#         cache_data((test_data, test_id), cache_path)\n",
    "#     else:\n",
    "#         print('Restore test from cache!')\n",
    "#         (test_data, test_id) = restore_data(cache_path)\n",
    "\n",
    "#     test_data = np.array(test_data, dtype=np.uint8)\n",
    "\n",
    "#     test_data = test_data.transpose((0, 3, 1, 2))\n",
    "\n",
    "#     test_data = test_data.astype('float32')\n",
    "#     mean_pixel = [103.939, 116.779, 123.68]\n",
    "#     for c in range(3):\n",
    "#         test_data[:, c, :, :] = test_data[:, c, :, :] - mean_pixel[c]\n",
    "#     # test_data /= 255\n",
    "#     print('Test shape:', test_data.shape)\n",
    "#     print(test_data.shape[0], 'test samples')\n",
    "#     return test_data, test_id\n",
    "\n",
    "\n",
    "def read_and_normalize_train_data():\n",
    "    train_data, train_target, train_id = load_train()\n",
    "\n",
    "    print('Convert to numpy...')\n",
    "    train_data = np.array(train_data, dtype=np.uint8)\n",
    "    train_target = np.array(train_target, dtype=np.uint8)\n",
    "\n",
    "    print('Reshape...')\n",
    "    train_data = train_data.transpose((0, 3, 1, 2))\n",
    "\n",
    "    print('Convert to float...')\n",
    "    train_data = train_data.astype('float32')\n",
    "    train_data = train_data / 255\n",
    "    train_target = np_utils.to_categorical(train_target, 8)\n",
    "\n",
    "    print('Train shape:', train_data.shape)\n",
    "    print(train_data.shape[0], 'train samples')\n",
    "    return train_data, train_target, train_id\n",
    "\n",
    "\n",
    "def read_and_normalize_test_data():\n",
    "    start_time = time.time()\n",
    "    test_data, test_id = load_test()\n",
    "\n",
    "    test_data = np.array(test_data, dtype=np.uint8)\n",
    "    test_data = test_data.transpose((0, 3, 1, 2))\n",
    "\n",
    "    test_data = test_data.astype('float32')\n",
    "    test_data = test_data / 255\n",
    "\n",
    "    print('Test shape:', test_data.shape)\n",
    "    print(test_data.shape[0], 'test samples')\n",
    "    print('Read and process test data time: {} seconds'.format(round(time.time() - start_time, 2)))\n",
    "    return test_data, test_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_cross_validation(nfolds=10, nb_epoch=10, split=0.2, modelStr='initial'):\n",
    "\n",
    "    # Now it loads color image\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = 64, 64\n",
    "    batch_size = 16\n",
    "    random_state = 20\n",
    "\n",
    "    train_data, train_target, train_id = read_and_normalize_train_data()\n",
    "\n",
    "#         read_and_normalize_and_shuffle_train_data(64, 64,3)\n",
    "        \n",
    "    # ishuf_train_data = []\n",
    "    # shuf_train_target = []\n",
    "    # index_shuf = range(len(train_target))\n",
    "    # shuffle(index_shuf)\n",
    "    # for i in index_shuf:\n",
    "    #     shuf_train_data.append(train_data[i])\n",
    "    #     shuf_train_target.append(train_target[i])\n",
    "\n",
    "    # yfull_train = dict()\n",
    "    # yfull_test = []\n",
    "    num_fold = 0\n",
    "    kf = KFold(len(train_target), n_folds=nfolds,\n",
    "               shuffle=True, random_state=random_state)\n",
    "    for train_imgs, test_imgs in kf:\n",
    "        num_fold += 1\n",
    "        print('Start KFold number {} from {}'.format(num_fold, nfolds))\n",
    "        # print('Split train: ', len(X_train), len(Y_train))\n",
    "        # print('Split valid: ', len(X_valid), len(Y_valid))\n",
    "        # print('Train drivers: ', unique_list_train)\n",
    "        # print('Test drivers: ', unique_list_valid)\n",
    "        # model = create_model_v1(img_rows, img_cols, color_type_global)\n",
    "        # model = vgg_bn_model(img_rows, img_cols, color_type_global)\n",
    "        model = vgg_std16_model(64, 64, 3)\n",
    "        # correct validation to kfold rather than singel fold\n",
    "        model.fit(train_data, train_target, batch_size=batch_size,\n",
    "                  nb_epoch=nb_epoch,\n",
    "                  show_accuracy=True, verbose=1,\n",
    "                  validation_split=split, shuffle=True)\n",
    "\n",
    "        # print('losses: ' + hist.history.losses[-1])\n",
    "\n",
    "        # print('Score log_loss: ', score[0])\n",
    "\n",
    "        save_model(model, num_fold, modelStr)\n",
    "\n",
    "        # predictions_valid = model.predict(X_valid, batch_size=128, verbose=1)\n",
    "        # score = log_loss(Y_valid, predictions_valid)\n",
    "        # print('Score log_loss: ', score)\n",
    "        # Store valid predictions\n",
    "        # for i in range(len(test_index)):\n",
    "        #    yfull_train[test_index[i]] = predictions_valid[i]\n",
    "\n",
    "    print('Start testing............')\n",
    "#     test_data, test_id = read_and_normalize_test_data(64, 64,3)\n",
    "    test_data, test_id = read_and_normalize_test_data()\n",
    "    yfull_test = []\n",
    "\n",
    "    for index in range(1, num_fold + 1):\n",
    "        # 1,2,3,4,5\n",
    "        # Store test predictions\n",
    "        model = read_model(index, modelStr)\n",
    "        test_prediction = model.predict(test_data, batch_size=128, verbose=1)\n",
    "        yfull_test.append(test_prediction)\n",
    "\n",
    "    info_string = 'loss_' + modelStr \\\n",
    "                  + '_r_' + str(img_rows) \\\n",
    "                  + '_c_' + str(img_cols) \\\n",
    "                  + '_folds_' + str(nfolds) \\\n",
    "                  + '_ep_' + str(nb_epoch)\n",
    "\n",
    "    test_res = merge_several_folds_mean(yfull_test, nfolds)\n",
    "    create_submission(test_res, test_id, info_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read train images\n",
      "Load folder ALB (Index: 0)\n",
      "Load folder BET (Index: 1)\n",
      "Load folder DOL (Index: 2)\n",
      "Load folder LAG (Index: 3)\n",
      "Load folder NoF (Index: 4)\n",
      "Load folder OTHER (Index: 5)\n",
      "Load folder SHARK (Index: 6)\n",
      "Load folder YFT (Index: 7)\n",
      "Read train data time: 60.5 seconds\n",
      "Convert to numpy...\n",
      "Reshape...\n",
      "Convert to float...\n",
      "('Train shape:', (3777, 3, 64, 64))\n",
      "(3777, 'train samples')\n",
      "Start KFold number 1 from 3\n",
      "model loading and compilation finished succesfully\n",
      "Train on 3210 samples, validate on 567 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape mismatch: x has 2048 cols (and 16 rows) but y has 25088 rows (and 4096 cols)\nApply node that caused the error: Dot22(Reshape{2}.0, dense_9_W)\nToposort index: 203\nInputs types: [TensorType(float32, matrix), TensorType(float32, matrix)]\nInputs shapes: [(16, 2048), (25088, 4096)]\nInputs strides: [(8192, 4), (16384, 4)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[Elemwise{add,no_inplace}(Dot22.0, InplaceDimShuffle{x,0}.0), Elemwise{Composite{Switch(i0, (Composite{(Abs(i0) + i1 + i2)}(i1, i2, i3) * i4), (i5 * Composite{(Abs(i0) + i1 + i2)}(i1, i2, i3)))}}[(0, 2)](InplaceDimShuffle{x,x}.0, Elemwise{add,no_inplace}.0, Dot22.0, InplaceDimShuffle{x,0}.0, Elemwise{Composite{Cast{float32}(LT(i0, i1))}}[(0, 0)].0, TensorConstant{(1, 1) of 0.5})]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-cd9d6ceb4e71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_image_dim_ordering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'th'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# nfolds, nb_epoch, split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrun_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_vgg_16_2x20'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# nb_epoch, split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-c08111c53a53>\u001b[0m in \u001b[0;36mrun_cross_validation\u001b[0;34m(nfolds, nb_epoch, split, modelStr)\u001b[0m\n\u001b[1;32m     38\u001b[0m                   \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                   \u001b[0mshow_accuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                   validation_split=split, shuffle=True)\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# print('losses: ' + hist.history.losses[-1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    650\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1109\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    824\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    869\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[1;32m    872\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[0;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;31m# extra long error message in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape mismatch: x has 2048 cols (and 16 rows) but y has 25088 rows (and 4096 cols)\nApply node that caused the error: Dot22(Reshape{2}.0, dense_9_W)\nToposort index: 203\nInputs types: [TensorType(float32, matrix), TensorType(float32, matrix)]\nInputs shapes: [(16, 2048), (25088, 4096)]\nInputs strides: [(8192, 4), (16384, 4)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[Elemwise{add,no_inplace}(Dot22.0, InplaceDimShuffle{x,0}.0), Elemwise{Composite{Switch(i0, (Composite{(Abs(i0) + i1 + i2)}(i1, i2, i3) * i4), (i5 * Composite{(Abs(i0) + i1 + i2)}(i1, i2, i3)))}}[(0, 2)](InplaceDimShuffle{x,x}.0, Elemwise{add,no_inplace}.0, Dot22.0, InplaceDimShuffle{x,0}.0, Elemwise{Composite{Cast{float32}(LT(i0, i1))}}[(0, 0)].0, TensorConstant{(1, 1) of 0.5})]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "import time\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "# nfolds, nb_epoch, split\n",
    "run_cross_validation(3, 20, 0.15, '_vgg_16_2x20')\n",
    "\n",
    "# nb_epoch, split\n",
    "# run_one_fold_cross_validation(10, 0.1)\n",
    "\n",
    "# test_model_and_submit(1, 10, 'high_epoch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
