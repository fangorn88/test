{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id               int64\n",
      "is_duplicate     int64\n",
      "qid1             int64\n",
      "qid2             int64\n",
      "question1       object\n",
      "question2       object\n",
      "test_id          int64\n",
      "dtype: object\n",
      "(2750086, 7)\n",
      "0    0.630802\n",
      "1    0.369198\n",
      "Name: is_duplicate, dtype: float64\n",
      "count    404290.000000\n",
      "mean         20.182436\n",
      "std          25.606319\n",
      "min           0.000000\n",
      "25%           5.000000\n",
      "50%          12.000000\n",
      "75%          26.000000\n",
      "max        1080.000000\n",
      "Name: abs_diff_len1_len2, dtype: float64\n",
      "Maximum among duplicates:        196.0\n",
      "Maximum among non-duplicates:      1080.0\n",
      "Maximum among non-duplicates:  397\n",
      "Standard deviation in duplicates: 14.3913\n",
      "New value:               224.782594681\n",
      "count    404290.000000\n",
      "mean          1.110986\n",
      "std           0.614168\n",
      "min           0.006711\n",
      "25%           0.793651\n",
      "50%           1.000000\n",
      "75%           1.272727\n",
      "max         117.000000\n",
      "Name: ratio_len1_len2, dtype: float64\n",
      "Maximum among duplicates:         6.66666666667\n",
      "Maximum among non-duplicates:       117.0\n",
      "Number of lines greater than threshold:  153\n",
      "Number of lines greater than threshold:  0.376162645193\n",
      "New value:                7.41899195705\n",
      "Unigrams: 1779\n",
      "Bigrams:  10722\n",
      "Trigrams: 74806\n",
      "Number of right speakers on the right: 0\n",
      "Number of outsiders on the left:  0\n",
      "0.370040460242\n",
      "2.18376447582e+12\n",
      "      fun: 8.4413330614142379e-05\n",
      " hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([ 0.00064175])\n",
      "  message: 'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 56\n",
      "      nit: 11\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([ 0.55365533])\n",
      "0.37065345216\n",
      "2.17951665163e+12\n",
      "      fun: 0.00010025039379553971\n",
      " hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([ 0.0020981])\n",
      "  message: 'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 60\n",
      "      nit: 10\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([ 0.51530244])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import  division\n",
    "\n",
    "#get_ipython().magic('matplotlib inline')\n",
    "#import matplotlib\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "#import seaborn as sns\n",
    "#sns.set_style(\"dark\")\n",
    "#plt.rcParams['figure.figsize'] = 16, 12\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import itertools as it\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import string\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss, make_scorer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# import eli5\n",
    "from IPython.display import display\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('train.csv', \n",
    "                       dtype={\n",
    "                           'question1': np.str,\n",
    "                           'question2': np.str\n",
    "                       })\n",
    "df_train['test_id'] = -1\n",
    "df_test = pd.read_csv('test.csv', \n",
    "                      dtype={\n",
    "                          'question1': np.str,\n",
    "                          'question2': np.str\n",
    "                      })\n",
    "df_test['id'] = -1\n",
    "df_test['qid1'] = -1\n",
    "df_test['qid2'] = -1\n",
    "df_test['is_duplicate'] = -1\n",
    "\n",
    "df = pd.concat([df_train, df_test])\n",
    "df['question1'] = df['question1'].fillna('')\n",
    "df['question2'] = df['question2'].fillna('')\n",
    "df['uid'] = np.arange(df.shape[0])\n",
    "df = df.set_index(['uid'])\n",
    "\n",
    "print df.dtypes\n",
    "del(df_train, df_test)\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "ix_train = np.where(df['id'] >= 0)[0]\n",
    "ix_test = np.where(df['id'] == -1)[0]\n",
    "ix_is_dup = np.where(df['is_duplicate'] == 1)[0]\n",
    "ix_not_dup = np.where(df['is_duplicate'] == 0)[0]\n",
    "\n",
    "\n",
    "print df.shape\n",
    "\n",
    "print df[df['is_duplicate'] >= 0]['is_duplicate'].value_counts(normalize=True)\n",
    "\n",
    "\n",
    "\n",
    "def link_function(x):\n",
    "    return gamma_1*x/(gamma_1*x + gamma_0*(1 - x))\n",
    "\n",
    "\n",
    "df['len1'] = df['question1'].str.len().astype(np.float32)\n",
    "df['len2'] = df['question2'].str.len().astype(np.float32)\n",
    "df['abs_diff_len1_len2'] = np.abs(df['len1'] - df['len2'])\n",
    "\n",
    "\n",
    "\n",
    "print df.loc[ix_train]['abs_diff_len1_len2'].describe()\n",
    "\n",
    "\n",
    "\n",
    "max_in_dup = df.loc[ix_is_dup]['abs_diff_len1_len2'].max()\n",
    "print 'Maximum among duplicates:       ', max_in_dup\n",
    "max_in_not_dups = df.loc[ix_not_dup]['abs_diff_len1_len2'].max()\n",
    "print 'Maximum among non-duplicates:     ', max_in_not_dups\n",
    "print 'Maximum among non-duplicates: ', (df.loc[ix_train]['abs_diff_len1_len2'] > max_in_dup).sum()\n",
    "std_in_dups = df.loc[ix_is_dup]['abs_diff_len1_len2'].std()\n",
    "print 'Standard deviation in duplicates:', std_in_dups\n",
    "replace_value = max_in_dup + 2*std_in_dups\n",
    "print 'New value:              ', replace_value\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "df['abs_diff_len1_len2'] = df['abs_diff_len1_len2'].apply(lambda x: x if x < replace_value else replace_value)\n",
    "# plot_real_feature('abs_diff_len1_len2')\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "df['log_abs_diff_len1_len2'] = np.log(df['abs_diff_len1_len2'] + 1)\n",
    "# plot_real_feature('log_abs_diff_len1_len2')\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "df['ratio_len1_len2'] = df['len1'].apply(lambda x: x if x > 0.0 else 1.0)/                        df['len2'].apply(lambda x: x if x > 0.0 else 1.0)\n",
    "\n",
    "print df.loc[ix_train]['ratio_len1_len2'].describe()\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "max_in_dup = df.loc[ix_is_dup]['ratio_len1_len2'].max()\n",
    "print 'Maximum among duplicates:        ', max_in_dup\n",
    "max_in_not_dups = df.loc[ix_not_dup]['ratio_len1_len2'].max()\n",
    "print 'Maximum among non-duplicates:      ', max_in_not_dups\n",
    "print 'Number of lines greater than threshold: ', (df.loc[ix_train]['ratio_len1_len2'] > max_in_dup).sum()\n",
    "std_in_dups = df.loc[ix_is_dup]['ratio_len1_len2'].std()\n",
    "print 'Number of lines greater than threshold: ', std_in_dups\n",
    "replace_value = max_in_dup + 2*std_in_dups\n",
    "print 'New value:               ', replace_value\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "df['ratio_len1_len2'] = df['ratio_len1_len2'].apply(lambda x: x if x < replace_value else replace_value)\n",
    "# plot_real_feature('ratio_len1_len2')\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "df['log_ratio_len1_len2'] = np.log(df['ratio_len1_len2'] + 1)\n",
    "# plot_real_feature('log_ratio_len1_len2')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if os.path.isfile('./../tmp/cv_char.pkl') and os.path.isfile('./../tmp/ch_freq.pkl'):\n",
    "with open('cv_char.pkl', 'rb') as f:\n",
    "    cv_char = pickle.load(f)\n",
    "with open('ch_freq.pkl', 'rb') as f:\n",
    "    ch_freq = pickle.load(f)\n",
    "\n",
    "\n",
    "unigrams = dict([(k, v) for (k, v) in cv_char.vocabulary_.items() if len(k) == 1])\n",
    "ix_unigrams = np.sort(unigrams.values())\n",
    "print 'Unigrams:', len(unigrams)\n",
    "bigrams = dict([(k, v) for (k, v) in cv_char.vocabulary_.items() if len(k) == 2])\n",
    "ix_bigrams = np.sort(bigrams.values())\n",
    "print 'Bigrams: ', len(bigrams)\n",
    "trigrams = dict([(k, v) for (k, v) in cv_char.vocabulary_.items() if len(k) == 3])\n",
    "ix_trigrams = np.sort(trigrams.values())\n",
    "print 'Trigrams:', len(trigrams)\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "def save_sparse_csr(fname, sm):\n",
    "    np.savez(fname, \n",
    "             data=sm.data, \n",
    "             indices=sm.indices,\n",
    "             indptr=sm.indptr, \n",
    "             shape=sm.shape)\n",
    "\n",
    "def load_sparse_csr(fname):\n",
    "    loader = np.load(fname)\n",
    "    return sparse.csr_matrix((\n",
    "        loader['data'], \n",
    "        loader['indices'], \n",
    "        loader['indptr']),\n",
    "        shape=loader['shape'])\n",
    "\n",
    "# if os.path.isfile('./../tmp/m_q1.npz') and os.path.isfile('./../tmp/m_q2.npz'):\n",
    "m_q1 = load_sparse_csr('m_q1.npz')\n",
    "m_q2 = load_sparse_csr('m_q2.npz')\n",
    "\n",
    "\n",
    "v_num = (m_q1[:, ix_unigrams] > 0).minimum((m_q2[:, ix_unigrams] > 0)).sum(axis=1)\n",
    "v_den = (m_q1[:, ix_unigrams] > 0).maximum((m_q2[:, ix_unigrams] > 0)).sum(axis=1)\n",
    "v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "\n",
    "df['unigram_jaccard'] = v_score\n",
    "# plot_real_feature('unigram_jaccard')\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "# We take into account each letter more than once\n",
    "v_num = m_q1[:, ix_unigrams].minimum(m_q2[:, ix_unigrams]).sum(axis=1)\n",
    "v_den = m_q1[:, ix_unigrams].sum(axis=1) + m_q2[:, ix_unigrams].sum(axis=1)\n",
    "v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "\n",
    "df['unigram_all_jaccard'] = v_score\n",
    "# plot_real_feature('unigram_all_jaccard')\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "# We take into account each letter more than once\n",
    "# Normalize the maximum value, and not the sum\n",
    "v_num = m_q1[:, ix_unigrams].minimum(m_q2[:, ix_unigrams]).sum(axis=1)\n",
    "v_den = m_q1[:, ix_unigrams].maximum(m_q2[:, ix_unigrams]).sum(axis=1)\n",
    "v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "\n",
    "df['unigram_all_jaccard_max'] = v_score\n",
    "# plot_real_feature('unigram_all_jaccard_max')\n",
    "\n",
    "\n",
    "# #### Bigrams\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "v_num = (m_q1[:, ix_bigrams] > 0).minimum((m_q2[:, ix_bigrams] > 0)).sum(axis=1)\n",
    "v_den = (m_q1[:, ix_bigrams] > 0).maximum((m_q2[:, ix_bigrams] > 0)).sum(axis=1)\n",
    "v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "\n",
    "df['bigram_jaccard'] = v_score\n",
    "\n",
    "\n",
    "print 'Number of right speakers on the right:', (df['bigram_jaccard'] > 1).sum()\n",
    "print 'Number of outsiders on the left: ', (df['bigram_jaccard'] < -1.47).sum()\n",
    "\n",
    "\n",
    "\n",
    "df.loc[df['bigram_jaccard'] < -1.478751, 'bigram_jaccard'] = -1.478751\n",
    "df.loc[df['bigram_jaccard'] > 1.0, 'bigram_jaccard'] = 1.0\n",
    "\n",
    "# We take into account each letter more than once\n",
    "v_num = m_q1[:, ix_bigrams].minimum(m_q2[:, ix_bigrams]).sum(axis=1)\n",
    "v_den = m_q1[:, ix_bigrams].sum(axis=1) + m_q2[:, ix_bigrams].sum(axis=1)\n",
    "v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "\n",
    "df['bigram_all_jaccard'] = v_score\n",
    "\n",
    "# Normalize the maximum value, and not the sum\n",
    "v_num = m_q1[:, ix_bigrams].minimum(m_q2[:, ix_bigrams]).sum(axis=1)\n",
    "v_den = m_q1[:, ix_bigrams].maximum(m_q2[:, ix_bigrams]).sum(axis=1)\n",
    "v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "\n",
    "df['bigram_all_jaccard_max'] = v_score\n",
    "\n",
    "\n",
    "m_q1 = m_q1[:, ix_trigrams]\n",
    "m_q2 = m_q2[:, ix_trigrams]\n",
    "\n",
    "\n",
    "\n",
    "v_num = (m_q1 > 0).minimum((m_q2 > 0)).sum(axis=1)\n",
    "v_den = (m_q1 > 0).maximum((m_q2 > 0)).sum(axis=1)\n",
    "v_den[np.where(v_den == 0)] = 1\n",
    "v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "\n",
    "df['trigram_jaccard'] = v_score\n",
    "# plot_real_feature('trigram_jaccard')\n",
    "\n",
    "\n",
    "v_num = m_q1.minimum(m_q2).sum(axis=1)\n",
    "v_den = m_q1.sum(axis=1) + m_q2.sum(axis=1)\n",
    "v_den[np.where(v_den == 0)] = 1\n",
    "v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "\n",
    "df['trigram_all_jaccard'] = v_score\n",
    "# plot_real_feature('trigram_all_jaccard')\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "# We take into account each letter more than once\n",
    "# Normalize the maximum value, and not the sum\n",
    "v_num = m_q1.minimum(m_q2).sum(axis=1)\n",
    "v_den = m_q1.maximum(m_q2).sum(axis=1)\n",
    "v_den[np.where(v_den == 0)] = 1\n",
    "v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "\n",
    "df['trigram_all_jaccard_max'] = v_score\n",
    "# plot_real_feature('trigram_all_jaccard_max')\n",
    "\n",
    "\n",
    "# ### Tf-idf and pair metrics on trigrams\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "tft = TfidfTransformer(\n",
    "    norm='l2', \n",
    "    use_idf=True, \n",
    "    smooth_idf=True, \n",
    "    sublinear_tf=False)\n",
    "\n",
    "tft = tft.fit(sparse.vstack((m_q1, m_q2)))\n",
    "m_q1_tf = tft.transform(m_q1)\n",
    "m_q2_tf = tft.transform(m_q2)\n",
    "\n",
    "v_num = np.array(m_q1_tf.multiply(m_q2_tf).sum(axis=1))[:, 0]\n",
    "v_den = np.array(np.sqrt(m_q1_tf.multiply(m_q1_tf).sum(axis=1)))[:, 0] *         np.array(np.sqrt(m_q2_tf.multiply(m_q2_tf).sum(axis=1)))[:, 0]\n",
    "v_num[np.where(v_den == 0)] = 1\n",
    "v_den[np.where(v_den == 0)] = 1\n",
    "\n",
    "v_score = 1 - v_num/v_den\n",
    "\n",
    "df['trigram_tfidf_cosine'] = v_score\n",
    "# plot_real_feature('trigram_tfidf_cosine')\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "tft = TfidfTransformer(\n",
    "    norm='l2', \n",
    "    use_idf=True, \n",
    "    smooth_idf=True, \n",
    "    sublinear_tf=False)\n",
    "\n",
    "tft = tft.fit(sparse.vstack((m_q1, m_q2)))\n",
    "m_q1_tf = tft.transform(m_q1)\n",
    "m_q2_tf = tft.transform(m_q2)\n",
    "\n",
    "v_score = (m_q1_tf - m_q2_tf)\n",
    "v_score = np.sqrt(np.array(v_score.multiply(v_score).sum(axis=1))[:, 0])\n",
    "\n",
    "df['trigram_tfidf_l2_euclidean'] = v_score\n",
    "# plot_real_feature('trigram_tfidf_l2_euclidean')\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "tft = TfidfTransformer(\n",
    "    norm='l1', \n",
    "    use_idf=True, \n",
    "    smooth_idf=True, \n",
    "    sublinear_tf=False)\n",
    "\n",
    "tft = tft.fit(sparse.vstack((m_q1, m_q2)))\n",
    "m_q1_tf = tft.transform(m_q1)\n",
    "m_q2_tf = tft.transform(m_q2)\n",
    "\n",
    "v_score = (m_q1_tf - m_q2_tf)\n",
    "v_score = np.sqrt(np.array(v_score.multiply(v_score).sum(axis=1))[:, 0])\n",
    "\n",
    "df['trigram_tfidf_l1_euclidean'] = v_score\n",
    "# plot_real_feature('trigram_tfidf_l1_euclidean')\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "tft = TfidfTransformer(\n",
    "    norm='l2', \n",
    "    use_idf=False, \n",
    "    smooth_idf=True, \n",
    "    sublinear_tf=False)\n",
    "\n",
    "tft = tft.fit(sparse.vstack((m_q1, m_q2)))\n",
    "m_q1_tf = tft.transform(m_q1)\n",
    "m_q2_tf = tft.transform(m_q2)\n",
    "\n",
    "v_score = (m_q1_tf - m_q2_tf)\n",
    "v_score = np.sqrt(np.array(v_score.multiply(v_score).sum(axis=1))[:, 0])\n",
    "\n",
    "df['trigram_tf_l2_euclidean'] = v_score\n",
    "# plot_real_feature('trigram_tf_l2_euclidean')\n",
    "\n",
    "\n",
    "\n",
    "def check_model(predictors, data=None, do_scaling=True):\n",
    "    classifier = lambda: SGDClassifier(\n",
    "        loss='log', \n",
    "        penalty='elasticnet', \n",
    "        fit_intercept=True, \n",
    "        n_iter=100, \n",
    "        shuffle=True, \n",
    "        n_jobs=-1,\n",
    "        class_weight=None)\n",
    "\n",
    "    steps = []\n",
    "    if do_scaling:\n",
    "        steps.append(('ss', StandardScaler()))\n",
    "    steps.append(('en', classifier()))\n",
    "    \n",
    "    model = Pipeline(steps=steps)\n",
    "\n",
    "    parameters = {\n",
    "        'en__alpha': [0.00001, 0.0001, 0.001, 0.01, 0.02, 0.1, 0.5, 0.9, 1],\n",
    "        'en__l1_ratio': [0, 0.0001, 0.001, 0.01, 0.1, 0.3, 0.5, 0.75, 0.9, 1]\n",
    "    }\n",
    "\n",
    "    folder = KFold(n_splits=5, shuffle=True,random_state = 2016)\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        parameters, \n",
    "        cv=folder, \n",
    "        n_jobs=-1, \n",
    "        verbose=1)\n",
    "    if data is None:\n",
    "        grid_search = grid_search.fit(df.loc[ix_train][predictors], \n",
    "                                      df.loc[ix_train]['is_duplicate'])\n",
    "    else:\n",
    "        grid_search = grid_search.fit(data['X'], \n",
    "                                      data['y'])\n",
    "    \n",
    "    return grid_search\n",
    "\n",
    "\n",
    "data = {\n",
    "    'X_train': sparse.csc_matrix(sparse.hstack((m_q1_tf, m_q2_tf)))[ix_train, :],\n",
    "    'y_train': df.loc[ix_train]['is_duplicate'],\n",
    "    'X_test': sparse.csc_matrix(sparse.hstack((m_q1_tf, m_q2_tf)))[ix_test, :],\n",
    "    'y_train_pred': np.zeros(ix_train.shape[0]),\n",
    "    'y_test_pred': []\n",
    "}\n",
    "\n",
    "\n",
    "with open('4_model_pred.pkl', 'rb') as f:\n",
    "    tmp = pickle.load(f)\n",
    "    model = tmp['model']\n",
    "    data['y_train_pred'] = tmp['y_train_pred']\n",
    "    data['y_test_pred'] = tmp['y_test_pred']\n",
    "    del(tmp)\n",
    "\n",
    "mp = np.mean(data['y_train_pred'])\n",
    "print mp\n",
    "\n",
    "def func(w):\n",
    "    return (mp*data['y_test_pred'].shape[0] - \n",
    "            np.sum(w[0]*data['y_test_pred']/(w[0]*data['y_test_pred'] + \n",
    "                                             (1 - w[0]) * (1 - data['y_test_pred']))))**2\n",
    "\n",
    "print func(np.array([1]))\n",
    "\n",
    "res = minimize(func, np.array([1]), method='L-BFGS-B', bounds=[(0, 1)])\n",
    "\n",
    "print res\n",
    "\n",
    "\n",
    "w = res['x'][0]\n",
    "\n",
    "def fix_function(x):\n",
    "    return w*x/(w*x + (1 - w)*(1 - x))\n",
    "\n",
    "support = np.linspace(0, 1, 1000)\n",
    "values = fix_function(support)\n",
    "\n",
    "data['y_test_pred_fixed'] = fix_function(data['y_test_pred'])\n",
    "\n",
    "\n",
    "df['m_q1_q2_tf_oof'] = np.zeros(df.shape[0])\n",
    "df.loc[ix_train, 'm_q1_q2_tf_oof'] = data['y_train_pred']\n",
    "df.loc[ix_test, 'm_q1_q2_tf_oof'] = data['y_test_pred_fixed']\n",
    "del(data)\n",
    "\n",
    "\n",
    "\n",
    "del(unigrams, bigrams, trigrams)\n",
    "\n",
    "\n",
    "\n",
    "with open('1_svd.pkl', 'rb') as f:\n",
    "    svd = pickle.load(f)\n",
    "with open('1_m_svd.npz', 'rb') as f:\n",
    "    m_svd = np.load(f)['arr_0']\n",
    "\n",
    "\n",
    "df['m_q1_q2_tf_svd0'] = m_svd[:, 0]\n",
    "\n",
    "df['m_q1_q2_tf_svd1'] = m_svd[:, 0]\n",
    "# plot_real_feature('m_q1_q2_tf_svd1')\n",
    "\n",
    "\n",
    "data={\n",
    "    'X_train': m_svd[ix_train, :],\n",
    "    'y_train': df.loc[ix_train]['is_duplicate'],\n",
    "    'X_test': m_svd[ix_test, :],\n",
    "    'y_train_pred': np.zeros(ix_train.shape[0]),\n",
    "    'y_test_pred': []\n",
    "}\n",
    "\n",
    "\n",
    "with open('7_model_pred.pkl', 'rb') as f:\n",
    "    tmp = pickle.load(f)\n",
    "    model = tmp['model']\n",
    "    data['y_train_pred'] = tmp['y_train_pred']\n",
    "    data['y_test_pred'] = tmp['y_test_pred']\n",
    "    del(tmp)\n",
    "\n",
    "\n",
    "mp = np.mean(data['y_train_pred'])\n",
    "print mp\n",
    "\n",
    "def func(w):\n",
    "    return (mp*data['y_test_pred'].shape[0] - \n",
    "            np.sum(w[0]*data['y_test_pred']/(w[0]*data['y_test_pred'] + \n",
    "                                             (1 - w[0]) * (1 - data['y_test_pred']))))**2\n",
    "\n",
    "print func(np.array([1]))\n",
    "\n",
    "res = minimize(func, np.array([1]), method='L-BFGS-B', bounds=[(0, 1)])\n",
    "\n",
    "print res\n",
    "\n",
    "\n",
    "w = res['x'][0]\n",
    "\n",
    "def fix_function(x):\n",
    "    return w*x/(w*x + (1 - w)*(1 - x))\n",
    "\n",
    "support = np.linspace(0, 1, 1000)\n",
    "values = fix_function(support)\n",
    "\n",
    "data['y_test_pred_fixed'] = fix_function(data['y_test_pred'])\n",
    "\n",
    "df['m_q1_q2_tf_svd100_oof'] = np.zeros(df.shape[0])\n",
    "df.loc[ix_train, 'm_q1_q2_tf_svd100_oof'] = data['y_train_pred']\n",
    "df.loc[ix_test, 'm_q1_q2_tf_svd100_oof'] = data['y_test_pred_fixed']\n",
    "del(data)\n",
    "\n",
    "\n",
    "del(m_q1, m_q2, m_svd)\n",
    "\n",
    "m_diff_q1_q2 = m_q1_tf - m_q2_tf\n",
    "\n",
    "\n",
    "data={\n",
    "    'X_train': m_diff_q1_q2[ix_train, :],\n",
    "    'y_train': df.loc[ix_train]['is_duplicate'],\n",
    "    'X_test': m_diff_q1_q2[ix_test, :],\n",
    "    'y_train_pred': np.zeros(ix_train.shape[0]),\n",
    "    'y_test_pred': []\n",
    "}\n",
    "\n",
    "\n",
    "with open('10_model_pred.pkl', 'rb') as f:\n",
    "    tmp = pickle.load(f)\n",
    "    model = tmp['model']\n",
    "    data['y_train_pred'] = tmp['y_train_pred']\n",
    "    data['y_test_pred'] = tmp['y_test_pred']\n",
    "    del(tmp)\n",
    "\n",
    "df['m_diff_q1_q2_tf_oof'] = np.zeros(df.shape[0])\n",
    "df.loc[ix_train, 'm_diff_q1_q2_tf_oof'] = data['y_train_pred']\n",
    "df.loc[ix_test, 'm_diff_q1_q2_tf_oof'] = data['y_test_pred']\n",
    "del(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'id', u'is_duplicate', u'qid1', u'qid2', u'question1', u'question2',\n",
       "       u'test_id', u'len1', u'len2', u'abs_diff_len1_len2',\n",
       "       u'log_abs_diff_len1_len2', u'ratio_len1_len2', u'log_ratio_len1_len2',\n",
       "       u'unigram_jaccard', u'unigram_all_jaccard', u'unigram_all_jaccard_max',\n",
       "       u'bigram_jaccard', u'bigram_all_jaccard', u'bigram_all_jaccard_max',\n",
       "       u'trigram_jaccard', u'trigram_all_jaccard', u'trigram_all_jaccard_max',\n",
       "       u'trigram_tfidf_cosine', u'trigram_tfidf_l2_euclidean',\n",
       "       u'trigram_tfidf_l1_euclidean', u'trigram_tf_l2_euclidean',\n",
       "       u'm_q1_q2_tf_oof', u'm_q1_q2_tf_svd0', u'm_q1_q2_tf_svd1',\n",
       "       u'm_q1_q2_tf_svd100_oof', u'm_diff_q1_q2_tf_oof'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_test = df[df.is_duplicate < 0 ]\n",
    "df_train = df[df.test_id == -1 ]\n",
    "\n",
    "np.savetxt('russ_tr.gz', np.array(df_train.iloc[:,9:]), delimiter=\",\", fmt='%.5f')\n",
    "np.savetxt('russ_ts.gz', np.array(df_test.iloc[:,9:]), delimiter=\",\", fmt='%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del(m_diff_q1_q2)\n",
    "\n",
    "\n",
    "# ## Normalized Validation\n",
    "# \n",
    "\n",
    "class QuoraKFold():\n",
    "    \n",
    "    def __init__(self, n_splits=10):\n",
    "        self.n_splits = n_splits\n",
    "        \n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "        \n",
    "    def split(self, X, y, groups=None):\n",
    "        r_split = 1 - 1.0/self.n_splits\n",
    "        k = r1*d[0]/(r0*d[1])\n",
    "        ix_ones = np.where(y == 1)[0]\n",
    "        ix_zeros = np.where(y == 0)[0]\n",
    "        for i in range(self.n_splits):\n",
    "            ix_first_zeros = np.random.choice(\n",
    "                ix_zeros, \n",
    "                size=int(r_split * ix_zeros.shape[0]), \n",
    "                replace=False)\n",
    "            ix_first_ones = np.random.choice(\n",
    "                ix_ones,\n",
    "                size=int(ix_first_zeros.shape[0] * d[1]/d[0]),\n",
    "                replace=False)\n",
    "            \n",
    "            ix_second_zeros = np.setdiff1d(ix_zeros, ix_first_zeros)\n",
    "            ix_second_ones = np.random.choice(\n",
    "                np.setdiff1d(ix_ones, ix_first_ones),\n",
    "                size=int(ix_second_zeros.shape[0] * r1/r0),\n",
    "                replace=False)\n",
    "            \n",
    "            ix_first = np.hstack((ix_first_zeros, ix_first_ones))\n",
    "            ix_first = ix_first[np.random.choice(\n",
    "                range(ix_first.shape[0]), size=ix_first.shape[0], replace=False)]\n",
    "            \n",
    "            ix_second = np.hstack((ix_second_zeros, ix_second_ones))\n",
    "            ix_second = ix_second[np.random.choice(\n",
    "                range(ix_second.shape[0]), size=ix_second.shape[0], replace=False)]\n",
    "            \n",
    "            yield ix_first, ix_second\n",
    "\n",
    "\n",
    "# In[61]:\n",
    "\n",
    "def log_loss_lf(y_true, y_pred):\n",
    "    return log_loss(y_true, link_function(y_pred[:, 1]), eps=eps)\n",
    "\n",
    "\n",
    "# In[62]:\n",
    "\n",
    "def check_model(predictors, data=None, do_scaling=True, folder=None):\n",
    "    classifier = lambda: SGDClassifier(\n",
    "        loss='log', \n",
    "        penalty='elasticnet', \n",
    "        fit_intercept=True, \n",
    "        n_iter=100, \n",
    "        shuffle=True, \n",
    "        n_jobs=-1,\n",
    "        class_weight=None)\n",
    "\n",
    "    steps = []\n",
    "    if do_scaling:\n",
    "        steps.append(('ss', StandardScaler()))\n",
    "    steps.append(('en', classifier()))\n",
    "    \n",
    "    model = Pipeline(steps=steps)\n",
    "\n",
    "    parameters = {\n",
    "        'en__alpha': [0.00001, 0.0001, 0.001, 0.01, 0.02, 0.1, 0.5, 0.9, 1],\n",
    "        'en__l1_ratio': [0, 0.0001, 0.001, 0.01, 0.1, 0.3, 0.5, 0.75, 0.9, 1]\n",
    "    }\n",
    "\n",
    "    if folder is None:\n",
    "        folder = QuoraKFold(n_splits=10)\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        parameters, \n",
    "        cv=folder, \n",
    "        n_jobs=-1,\n",
    "        scoring=make_scorer(log_loss_lf, greater_is_better=False, needs_proba=True),\n",
    "        verbose=1)\n",
    "    if data is None:\n",
    "        grid_search = grid_search.fit(df.loc[ix_train][predictors], \n",
    "                                      df.loc[ix_train]['is_duplicate'])\n",
    "    else:\n",
    "        grid_search = grid_search.fit(data['X'], \n",
    "                                      data['y'])\n",
    "    \n",
    "    return grid_search\n",
    "\n",
    "\n",
    "if not os.path.isfile('./../tmp/2_svd.pkl'):\n",
    "    svd = TruncatedSVD(n_components=100)\n",
    "    m_svd = svd.fit_transform(sparse.csc_matrix(sparse.vstack((m_q1_tf, m_q2_tf))))\n",
    "    with open('2_svd.pkl', 'wb') as f:\n",
    "        pickle.dump(svd, f)\n",
    "    with open('2_m_svd.npz', 'wb') as f:\n",
    "        np.savez(f, m_svd)\n",
    "else:\n",
    "    with open('./../tmp/2_svd.pkl', 'rb') as f:\n",
    "        svd = pickle.load(f)\n",
    "    with open('./../tmp/2_m_svd.npz', 'rb') as f:\n",
    "        m_svd = np.load(f)['arr_0']\n",
    "\n",
    "\n",
    "del(m_q1_tf, m_q2_tf)\n",
    "\n",
    "\n",
    "m_svd_q1 = m_svd[:m_svd.shape[0]/2, :]\n",
    "m_svd_q2 = m_svd[m_svd.shape[0]/2:, :]\n",
    "del(m_svd)\n",
    "\n",
    "\n",
    "# In[128]:\n",
    "\n",
    "df['m_vstack_svd_q1_q1_euclidean'] = ((m_svd_q1 - m_svd_q2)**2).mean(axis=1)\n",
    "\n",
    "\n",
    "num = (m_svd_q1*m_svd_q2).sum(axis=1)\n",
    "den = np.sqrt((m_svd_q1**2).sum(axis=1))*np.sqrt((m_svd_q2**2).sum(axis=1))\n",
    "num[np.where(den == 0)] = 0\n",
    "den[np.where(den == 0)] = 1\n",
    "df['m_vstack_svd_q1_q1_cosine'] = 1 - num/den\n",
    "\n",
    "\n",
    "m_svd = m_svd_q1*m_svd_q2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id               int64\n",
      "is_duplicate     int64\n",
      "qid1             int64\n",
      "qid2             int64\n",
      "question1       object\n",
      "question2       object\n",
      "test_id          int64\n",
      "dtype: object\n",
      "(2750086, 7)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import  division\n",
    "\n",
    "#get_ipython().magic('matplotlib inline')\n",
    "#import matplotlib\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "#import seaborn as sns\n",
    "#sns.set_style(\"dark\")\n",
    "#plt.rcParams['figure.figsize'] = 16, 12\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import itertools as it\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import string\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss, make_scorer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# import eli5\n",
    "from IPython.display import display\n",
    "\n",
    "import xgboost as xgb\n",
    "df_train = pd.read_csv('train.csv', \n",
    "                       dtype={\n",
    "                           'question1': np.str,\n",
    "                           'question2': np.str\n",
    "                       })\n",
    "df_train['test_id'] = -1\n",
    "df_test = pd.read_csv('test.csv', \n",
    "                      dtype={\n",
    "                          'question1': np.str,\n",
    "                          'question2': np.str\n",
    "                      })\n",
    "df_test['id'] = -1\n",
    "df_test['qid1'] = -1\n",
    "df_test['qid2'] = -1\n",
    "df_test['is_duplicate'] = -1\n",
    "\n",
    "df = pd.concat([df_train, df_test])\n",
    "df['question1'] = df['question1'].fillna('')\n",
    "df['question2'] = df['question2'].fillna('')\n",
    "df['uid'] = np.arange(df.shape[0])\n",
    "df = df.set_index(['uid'])\n",
    "\n",
    "print df.dtypes\n",
    "del(df_train, df_test)\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "ix_train = np.where(df['id'] >= 0)[0]\n",
    "ix_test = np.where(df['id'] == -1)[0]\n",
    "ix_is_dup = np.where(df['is_duplicate'] == 1)[0]\n",
    "ix_not_dup = np.where(df['is_duplicate'] == 0)[0]\n",
    "\n",
    "\n",
    "print df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def save_sparse_csr(fname, sm):\n",
    "    np.savez(fname, \n",
    "             data=sm.data, \n",
    "             indices=sm.indices,\n",
    "             indptr=sm.indptr, \n",
    "             shape=sm.shape)\n",
    "\n",
    "def load_sparse_csr(fname):\n",
    "    loader = np.load(fname)\n",
    "    return sparse.csr_matrix((\n",
    "        loader['data'], \n",
    "        loader['indices'], \n",
    "        loader['indptr']),\n",
    "        shape=loader['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en_core_web_sm')\n",
    "# df.head()['question1'].apply(lambda s: ' '.join([c.lemma_ for c in nlp(unicode(s)) if c.lemma_  != '?']))\n",
    "\n",
    "\n",
    "\n",
    "# SYMBOLS = set(' '.join(string.punctuation).split(' ') + ['...', '\"', '\"', '\\'ve'])\n",
    "\n",
    "\n",
    "# if not os.path.isfile('./../tmp/bow_lemma.pkl'):\n",
    "#     q1 = []\n",
    "\n",
    "#     for doc in nlp.pipe(df['question1'].str.decode('utf-8'), n_threads=16, batch_size=10000):\n",
    "#         q1.append([c.lemma_ for c in doc if c.lemma_ not in SYMBOLS])\n",
    "\n",
    "#     q2 = []\n",
    "\n",
    "#     for doc in nlp.pipe(df['question2'].str.decode('utf-8'), n_threads=16, batch_size=10000):\n",
    "#         q2.append([c.lemma_ for c in doc if c.lemma_ not in SYMBOLS])\n",
    "        \n",
    "#     with open('bow_lemma.pkl', 'wb') as f:\n",
    "#         pickle.dump({\n",
    "#             'q1': q1,\n",
    "#             'q2': q2\n",
    "#         }, f)\n",
    "# else:\n",
    "#     with open('./../tmp/bow_lemma.pkl', 'rb') as f:\n",
    "#         tmp = pickle.load(f)\n",
    "#         q1 = tmp['q1']\n",
    "#         q2 = tmp['q2']\n",
    "#         del(tmp)\n",
    "\n",
    "\n",
    "# if os.path.isfile('./../tmp/cv_word_lemma.pkl') and os.path.isfile('./../tmp/wl_freq.pkl'):\n",
    "#     with open('./../tmp/cv_word_lemma.pkl', 'rb') as f:\n",
    "#         cv_words = pickle.load(f)\n",
    "#     with open('./../tmp/wl_freq.pkl', 'rb') as f:\n",
    "#         w_freq = pickle.load(f)\n",
    "# else:\n",
    "#     cv_words = CountVectorizer(ngram_range=(1, 1), analyzer='word')\n",
    "#     w_freq = np.array(cv_words.fit_transform(\n",
    "#         [' '.join(s) for s in q1] + [' '.join(s) for s in q2]).sum(axis=0))[0, :]\n",
    "#     with open('cv_word_lemma.pkl', 'wb') as f:\n",
    "#         pickle.dump(cv_words, f)\n",
    "#     with open('wl_freq.pkl', 'wb') as f:\n",
    "#         pickle.dump(w_freq, f)\n",
    "\n",
    "# # In[149]:\n",
    "\n",
    "if os.path.isfile('./../tmp/m_q1_wl.npz') and os.path.isfile('./../tmp/m_q2_wl.npz'):\n",
    "    m_q1 = load_sparse_csr('./../tmp/m_q1_wl.npz')\n",
    "    m_q2 = load_sparse_csr('./../tmp/m_q2_wl.npz')\n",
    "else:\n",
    "    m_q1 = cv_words.transform([' '.join(s) for s in q1])\n",
    "    m_q2 = cv_words.transform([' '.join(s) for s in q2])\n",
    "    save_sparse_csr('m_q1_wl.npz', m_q1)\n",
    "    save_sparse_csr('m_q2_wl.npz', m_q2)\n",
    "\n",
    "\n",
    "tft = TfidfTransformer(\n",
    "    norm='l2', \n",
    "    use_idf=True, \n",
    "    smooth_idf=True, \n",
    "    sublinear_tf=False)\n",
    "\n",
    "tft = tft.fit(sparse.vstack((m_q1, m_q2)))\n",
    "m_q1_tf = tft.transform(m_q1)\n",
    "m_q2_tf = tft.transform(m_q2)\n",
    "\n",
    "v_num = np.array(m_q1_tf.multiply(m_q2_tf).sum(axis=1))[:, 0]\n",
    "v_den = np.array(np.sqrt(m_q1_tf.multiply(m_q1_tf).sum(axis=1)))[:, 0] *         np.array(np.sqrt(m_q2_tf.multiply(m_q2_tf).sum(axis=1)))[:, 0]\n",
    "v_num[np.where(v_den == 0)] = 1\n",
    "v_den[np.where(v_den == 0)] = 1\n",
    "\n",
    "v_score = 1 - v_num/v_den\n",
    "\n",
    "df['1wl_tfidf_cosine'] = v_score\n",
    "\n",
    "\n",
    "# In[151]:\n",
    "\n",
    "tft = TfidfTransformer(\n",
    "    norm='l2', \n",
    "    use_idf=True, \n",
    "    smooth_idf=True, \n",
    "    sublinear_tf=False)\n",
    "\n",
    "tft = tft.fit(sparse.vstack((m_q1, m_q2)))\n",
    "m_q1_tf = tft.transform(m_q1)\n",
    "m_q2_tf = tft.transform(m_q2)\n",
    "\n",
    "v_score = (m_q1_tf - m_q2_tf)\n",
    "v_score = np.sqrt(np.array(v_score.multiply(v_score).sum(axis=1))[:, 0])\n",
    "\n",
    "df['1wl_tfidf_l2_euclidean'] = v_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838b47f77d004f1da1e43bc403401468"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.370212197344\n",
      "2.18257397948e+12\n",
      "      fun: 8.1708271521497991e-05\n",
      " hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([ 0.00926032])\n",
      "  message: 'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 54\n",
      "      nit: 9\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([ 0.5623325])\n"
     ]
    }
   ],
   "source": [
    "tft = TfidfTransformer(\n",
    "    norm='l2', \n",
    "    use_idf=False, \n",
    "    smooth_idf=True, \n",
    "    sublinear_tf=False)\n",
    "\n",
    "tft = tft.fit(sparse.vstack((m_q1, m_q2)))\n",
    "m_q1_tf = tft.transform(m_q1)\n",
    "m_q2_tf = tft.transform(m_q2)\n",
    "\n",
    "v_score = (m_q1_tf - m_q2_tf)\n",
    "v_score = np.sqrt(np.array(v_score.multiply(v_score).sum(axis=1))[:, 0])\n",
    "\n",
    "df['1wl_tf_l2_euclidean'] = v_score\n",
    "\n",
    "\n",
    "# In[153]:\n",
    "\n",
    "tft = TfidfTransformer(\n",
    "    norm='l2', \n",
    "    use_idf=True, \n",
    "    smooth_idf=True, \n",
    "    sublinear_tf=False)\n",
    "\n",
    "tft = tft.fit(sparse.vstack((m_q1, m_q2)))\n",
    "m_q1_tf = tft.transform(m_q1)\n",
    "m_q2_tf = tft.transform(m_q2)\n",
    "\n",
    "\n",
    "data={\n",
    "    'X_train': sparse.csc_matrix(sparse.hstack((m_q1_tf, m_q2_tf)))[ix_train, :],\n",
    "    'y_train': df.loc[ix_train]['is_duplicate'],\n",
    "    'X_test': sparse.csc_matrix(sparse.hstack((m_q1_tf, m_q2_tf)))[ix_test, :],\n",
    "    'y_train_pred': np.zeros(ix_train.shape[0]),\n",
    "    'y_test_pred': []\n",
    "}\n",
    "del(m_q1_tf, m_q2_tf)\n",
    "\n",
    "\n",
    "if not os.path.isfile('./../tmp/21_model_pred.pkl'):\n",
    "    n_splits = 10\n",
    "    folder = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "    for ix_first, ix_second in tqdm_notebook(folder.split(np.zeros(data['y_train'].shape[0]), data['y_train']), \n",
    "                                             total=n_splits):\n",
    "        # {'en__l1_ratio': 0.0001, 'en__alpha': 1e-05}\n",
    "        model = SGDClassifier(\n",
    "            loss='log', \n",
    "            penalty='elasticnet', \n",
    "            fit_intercept=True, \n",
    "            n_iter=100, \n",
    "            shuffle=True, \n",
    "            n_jobs=-1,\n",
    "            l1_ratio=0.0001,\n",
    "            alpha=1e-05,\n",
    "            class_weight=None)\n",
    "        model = model.fit(data['X_train'][ix_first, :], data['y_train'][ix_first])\n",
    "        data['y_train_pred'][ix_second] = model.predict_proba(data['X_train'][ix_second, :])[:, 1]\n",
    "        data['y_test_pred'].append(model.predict_proba(data['X_test'])[:, 1])\n",
    "        \n",
    "    data['y_test_pred'] = np.array(data['y_test_pred']).T.mean(axis=1)\n",
    "    with open('21_model_pred.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'model': model,\n",
    "            'y_train_pred': data['y_train_pred'],\n",
    "            'y_test_pred': data['y_test_pred']\n",
    "        }, f)\n",
    "else:\n",
    "    with open('./../tmp/21_model_pred.pkl', 'rb') as f:\n",
    "        tmp = pickle.load(f)\n",
    "        model = tmp['model']\n",
    "        data['y_train_pred'] = tmp['y_train_pred']\n",
    "        data['y_test_pred'] = tmp['y_test_pred']\n",
    "        del(tmp)\n",
    "\n",
    "\n",
    "\n",
    "mp = np.mean(data['y_train_pred'])\n",
    "print mp\n",
    "\n",
    "def func(w):\n",
    "    return (mp*data['y_test_pred'].shape[0] - \n",
    "            np.sum(w[0]*data['y_test_pred']/(w[0]*data['y_test_pred'] + \n",
    "                                             (1 - w[0]) * (1 - data['y_test_pred']))))**2\n",
    "\n",
    "print func(np.array([1]))\n",
    "\n",
    "res = minimize(func, np.array([1]), method='L-BFGS-B', bounds=[(0, 1)])\n",
    "\n",
    "print res\n",
    "\n",
    "\n",
    "# In[160]:\n",
    "\n",
    "w = res['x'][0]\n",
    "\n",
    "def fix_function(x):\n",
    "    return w*x/(w*x + (1 - w)*(1 - x))\n",
    "\n",
    "support = np.linspace(0, 1, 1000)\n",
    "values = fix_function(support)\n",
    "\n",
    "\n",
    "data['y_test_pred_fixed'] = fix_function(data['y_test_pred'])\n",
    "\n",
    "\n",
    "\n",
    "df['m_w1l_tfidf_oof'] = np.zeros(df.shape[0])\n",
    "df.loc[ix_train, 'm_w1l_tfidf_oof'] = data['y_train_pred']\n",
    "df.loc[ix_test, 'm_w1l_tfidf_oof'] = data['y_test_pred_fixed']\n",
    "del(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_test = df[df.is_duplicate < 0 ]\n",
    "df_train = df[df.test_id == -1 ]\n",
    "\n",
    "np.savetxt('russpacy_tr.gz', np.array(df_train.iloc[:,7:]), delimiter=\",\", fmt='%.5f')\n",
    "np.savetxt('russpacy_ts.gz', np.array(df_test.iloc[:,7:]), delimiter=\",\", fmt='%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1wl_tfidf_cosine</th>\n",
       "      <th>1wl_tfidf_l2_euclidean</th>\n",
       "      <th>1wl_tf_l2_euclidean</th>\n",
       "      <th>m_w1l_tfidf_oof</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.024289</td>\n",
       "      <td>0.220406</td>\n",
       "      <td>0.331930</td>\n",
       "      <td>0.245007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.221008</td>\n",
       "      <td>0.664843</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.205974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.602730</td>\n",
       "      <td>1.097935</td>\n",
       "      <td>1.087889</td>\n",
       "      <td>0.535736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.995384</td>\n",
       "      <td>1.410946</td>\n",
       "      <td>1.374306</td>\n",
       "      <td>0.565470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.740260</td>\n",
       "      <td>1.216767</td>\n",
       "      <td>1.077670</td>\n",
       "      <td>0.263488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.509998</td>\n",
       "      <td>1.009949</td>\n",
       "      <td>0.774597</td>\n",
       "      <td>0.197722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>0.143562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.220706</td>\n",
       "      <td>0.664388</td>\n",
       "      <td>1.064882</td>\n",
       "      <td>0.451188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.039160</td>\n",
       "      <td>0.279857</td>\n",
       "      <td>0.385175</td>\n",
       "      <td>0.067901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.368869</td>\n",
       "      <td>0.858917</td>\n",
       "      <td>0.997478</td>\n",
       "      <td>0.374277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.980015</td>\n",
       "      <td>1.400011</td>\n",
       "      <td>1.316561</td>\n",
       "      <td>0.210105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.384079</td>\n",
       "      <td>0.876446</td>\n",
       "      <td>0.815569</td>\n",
       "      <td>0.329822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.040022</td>\n",
       "      <td>0.282919</td>\n",
       "      <td>0.629629</td>\n",
       "      <td>0.653365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.060536</td>\n",
       "      <td>0.347955</td>\n",
       "      <td>0.385175</td>\n",
       "      <td>0.608693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.061095</td>\n",
       "      <td>0.349558</td>\n",
       "      <td>0.210819</td>\n",
       "      <td>0.106079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.756329</td>\n",
       "      <td>1.229902</td>\n",
       "      <td>1.281184</td>\n",
       "      <td>0.553822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.365395</td>\n",
       "      <td>0.854863</td>\n",
       "      <td>1.061663</td>\n",
       "      <td>0.146206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.623649</td>\n",
       "      <td>1.116825</td>\n",
       "      <td>1.014532</td>\n",
       "      <td>0.897073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.494232</td>\n",
       "      <td>0.994216</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.653902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.428725</td>\n",
       "      <td>0.925986</td>\n",
       "      <td>0.992882</td>\n",
       "      <td>0.395194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.333543</td>\n",
       "      <td>0.816753</td>\n",
       "      <td>0.857373</td>\n",
       "      <td>0.430425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.161219</td>\n",
       "      <td>0.567837</td>\n",
       "      <td>0.860301</td>\n",
       "      <td>0.780501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>0.192580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.697602</td>\n",
       "      <td>1.181187</td>\n",
       "      <td>1.255283</td>\n",
       "      <td>0.401582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.356987</td>\n",
       "      <td>0.844970</td>\n",
       "      <td>0.453684</td>\n",
       "      <td>0.100403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.248851</td>\n",
       "      <td>0.705479</td>\n",
       "      <td>0.605811</td>\n",
       "      <td>0.299437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.508840</td>\n",
       "      <td>1.008801</td>\n",
       "      <td>1.094631</td>\n",
       "      <td>0.250640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.238540</td>\n",
       "      <td>0.690710</td>\n",
       "      <td>0.591679</td>\n",
       "      <td>0.875031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.348133</td>\n",
       "      <td>0.834425</td>\n",
       "      <td>0.734443</td>\n",
       "      <td>0.731950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750056</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>0.207751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750057</th>\n",
       "      <td>0.679076</td>\n",
       "      <td>1.165398</td>\n",
       "      <td>1.261646</td>\n",
       "      <td>0.152628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750058</th>\n",
       "      <td>0.513938</td>\n",
       "      <td>1.013843</td>\n",
       "      <td>1.143803</td>\n",
       "      <td>0.235361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750059</th>\n",
       "      <td>0.981557</td>\n",
       "      <td>1.401112</td>\n",
       "      <td>1.297602</td>\n",
       "      <td>0.292495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750060</th>\n",
       "      <td>0.268373</td>\n",
       "      <td>0.732630</td>\n",
       "      <td>0.796078</td>\n",
       "      <td>0.790446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750061</th>\n",
       "      <td>0.410493</td>\n",
       "      <td>0.906083</td>\n",
       "      <td>0.964860</td>\n",
       "      <td>0.291308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750062</th>\n",
       "      <td>0.671802</td>\n",
       "      <td>1.159140</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.088390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750063</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>0.855245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750064</th>\n",
       "      <td>0.829281</td>\n",
       "      <td>1.287852</td>\n",
       "      <td>1.316335</td>\n",
       "      <td>0.565518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750065</th>\n",
       "      <td>0.330965</td>\n",
       "      <td>0.813590</td>\n",
       "      <td>0.842025</td>\n",
       "      <td>0.238457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750066</th>\n",
       "      <td>0.475136</td>\n",
       "      <td>0.974819</td>\n",
       "      <td>1.028198</td>\n",
       "      <td>0.855620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750067</th>\n",
       "      <td>0.822854</td>\n",
       "      <td>1.282852</td>\n",
       "      <td>1.087889</td>\n",
       "      <td>0.536904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750068</th>\n",
       "      <td>0.528146</td>\n",
       "      <td>1.027761</td>\n",
       "      <td>1.092784</td>\n",
       "      <td>0.331477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750069</th>\n",
       "      <td>0.608706</td>\n",
       "      <td>1.103364</td>\n",
       "      <td>1.044466</td>\n",
       "      <td>0.258721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750070</th>\n",
       "      <td>0.521501</td>\n",
       "      <td>1.021275</td>\n",
       "      <td>1.181938</td>\n",
       "      <td>0.180769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750071</th>\n",
       "      <td>0.688540</td>\n",
       "      <td>1.173490</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.230082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750072</th>\n",
       "      <td>0.441688</td>\n",
       "      <td>0.939881</td>\n",
       "      <td>1.160518</td>\n",
       "      <td>0.844460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750073</th>\n",
       "      <td>0.722682</td>\n",
       "      <td>1.202233</td>\n",
       "      <td>0.780558</td>\n",
       "      <td>0.039071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750074</th>\n",
       "      <td>0.046590</td>\n",
       "      <td>0.305254</td>\n",
       "      <td>0.385175</td>\n",
       "      <td>0.569678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750075</th>\n",
       "      <td>0.983660</td>\n",
       "      <td>1.402612</td>\n",
       "      <td>1.322128</td>\n",
       "      <td>0.461248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750076</th>\n",
       "      <td>0.671343</td>\n",
       "      <td>1.158743</td>\n",
       "      <td>1.094631</td>\n",
       "      <td>0.289810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750077</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>0.214325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750078</th>\n",
       "      <td>0.728042</td>\n",
       "      <td>1.206683</td>\n",
       "      <td>1.032890</td>\n",
       "      <td>0.415120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750079</th>\n",
       "      <td>0.955804</td>\n",
       "      <td>1.382609</td>\n",
       "      <td>1.162068</td>\n",
       "      <td>0.191170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750080</th>\n",
       "      <td>0.878520</td>\n",
       "      <td>1.325534</td>\n",
       "      <td>1.138128</td>\n",
       "      <td>0.150560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750081</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>0.309901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750082</th>\n",
       "      <td>0.659465</td>\n",
       "      <td>1.148447</td>\n",
       "      <td>1.304553</td>\n",
       "      <td>0.064764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750083</th>\n",
       "      <td>0.516018</td>\n",
       "      <td>1.015892</td>\n",
       "      <td>1.043224</td>\n",
       "      <td>0.190321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750084</th>\n",
       "      <td>0.304027</td>\n",
       "      <td>0.779779</td>\n",
       "      <td>0.426401</td>\n",
       "      <td>0.464199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750085</th>\n",
       "      <td>0.410367</td>\n",
       "      <td>0.905943</td>\n",
       "      <td>1.236364</td>\n",
       "      <td>0.572202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2750086 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         1wl_tfidf_cosine  1wl_tfidf_l2_euclidean  1wl_tf_l2_euclidean  \\\n",
       "uid                                                                      \n",
       "0                0.024289                0.220406             0.331930   \n",
       "1                0.221008                0.664843             0.912871   \n",
       "2                0.602730                1.097935             1.087889   \n",
       "3                0.995384                1.410946             1.374306   \n",
       "4                0.740260                1.216767             1.077670   \n",
       "5                0.509998                1.009949             0.774597   \n",
       "6                1.000000                1.414214             1.414214   \n",
       "7                0.220706                0.664388             1.064882   \n",
       "8                0.039160                0.279857             0.385175   \n",
       "9                0.368869                0.858917             0.997478   \n",
       "10               0.980015                1.400011             1.316561   \n",
       "11               0.384079                0.876446             0.815569   \n",
       "12               0.040022                0.282919             0.629629   \n",
       "13               0.060536                0.347955             0.385175   \n",
       "14               0.061095                0.349558             0.210819   \n",
       "15               0.756329                1.229902             1.281184   \n",
       "16               0.000000                0.000000             0.000000   \n",
       "17               0.365395                0.854863             1.061663   \n",
       "18               0.623649                1.116825             1.014532   \n",
       "19               0.494232                0.994216             0.666667   \n",
       "20               0.428725                0.925986             0.992882   \n",
       "21               0.333543                0.816753             0.857373   \n",
       "22               0.161219                0.567837             0.860301   \n",
       "23               1.000000                1.414214             1.414214   \n",
       "24               0.697602                1.181187             1.255283   \n",
       "25               0.356987                0.844970             0.453684   \n",
       "26               0.248851                0.705479             0.605811   \n",
       "27               0.508840                1.008801             1.094631   \n",
       "28               0.238540                0.690710             0.591679   \n",
       "29               0.348133                0.834425             0.734443   \n",
       "...                   ...                     ...                  ...   \n",
       "2750056          1.000000                1.414214             1.414214   \n",
       "2750057          0.679076                1.165398             1.261646   \n",
       "2750058          0.513938                1.013843             1.143803   \n",
       "2750059          0.981557                1.401112             1.297602   \n",
       "2750060          0.268373                0.732630             0.796078   \n",
       "2750061          0.410493                0.906083             0.964860   \n",
       "2750062          0.671802                1.159140             0.707107   \n",
       "2750063          1.000000                1.414214             1.414214   \n",
       "2750064          0.829281                1.287852             1.316335   \n",
       "2750065          0.330965                0.813590             0.842025   \n",
       "2750066          0.475136                0.974819             1.028198   \n",
       "2750067          0.822854                1.282852             1.087889   \n",
       "2750068          0.528146                1.027761             1.092784   \n",
       "2750069          0.608706                1.103364             1.044466   \n",
       "2750070          0.521501                1.021275             1.181938   \n",
       "2750071          0.688540                1.173490             1.000000   \n",
       "2750072          0.441688                0.939881             1.160518   \n",
       "2750073          0.722682                1.202233             0.780558   \n",
       "2750074          0.046590                0.305254             0.385175   \n",
       "2750075          0.983660                1.402612             1.322128   \n",
       "2750076          0.671343                1.158743             1.094631   \n",
       "2750077          1.000000                1.414214             1.414214   \n",
       "2750078          0.728042                1.206683             1.032890   \n",
       "2750079          0.955804                1.382609             1.162068   \n",
       "2750080          0.878520                1.325534             1.138128   \n",
       "2750081          1.000000                1.414214             1.414214   \n",
       "2750082          0.659465                1.148447             1.304553   \n",
       "2750083          0.516018                1.015892             1.043224   \n",
       "2750084          0.304027                0.779779             0.426401   \n",
       "2750085          0.410367                0.905943             1.236364   \n",
       "\n",
       "         m_w1l_tfidf_oof  \n",
       "uid                       \n",
       "0               0.245007  \n",
       "1               0.205974  \n",
       "2               0.535736  \n",
       "3               0.565470  \n",
       "4               0.263488  \n",
       "5               0.197722  \n",
       "6               0.143562  \n",
       "7               0.451188  \n",
       "8               0.067901  \n",
       "9               0.374277  \n",
       "10              0.210105  \n",
       "11              0.329822  \n",
       "12              0.653365  \n",
       "13              0.608693  \n",
       "14              0.106079  \n",
       "15              0.553822  \n",
       "16              0.151971  \n",
       "17              0.146206  \n",
       "18              0.897073  \n",
       "19              0.653902  \n",
       "20              0.395194  \n",
       "21              0.430425  \n",
       "22              0.780501  \n",
       "23              0.192580  \n",
       "24              0.401582  \n",
       "25              0.100403  \n",
       "26              0.299437  \n",
       "27              0.250640  \n",
       "28              0.875031  \n",
       "29              0.731950  \n",
       "...                  ...  \n",
       "2750056         0.207751  \n",
       "2750057         0.152628  \n",
       "2750058         0.235361  \n",
       "2750059         0.292495  \n",
       "2750060         0.790446  \n",
       "2750061         0.291308  \n",
       "2750062         0.088390  \n",
       "2750063         0.855245  \n",
       "2750064         0.565518  \n",
       "2750065         0.238457  \n",
       "2750066         0.855620  \n",
       "2750067         0.536904  \n",
       "2750068         0.331477  \n",
       "2750069         0.258721  \n",
       "2750070         0.180769  \n",
       "2750071         0.230082  \n",
       "2750072         0.844460  \n",
       "2750073         0.039071  \n",
       "2750074         0.569678  \n",
       "2750075         0.461248  \n",
       "2750076         0.289810  \n",
       "2750077         0.214325  \n",
       "2750078         0.415120  \n",
       "2750079         0.191170  \n",
       "2750080         0.150560  \n",
       "2750081         0.309901  \n",
       "2750082         0.064764  \n",
       "2750083         0.190321  \n",
       "2750084         0.464199  \n",
       "2750085         0.572202  \n",
       "\n",
       "[2750086 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'id', u'is_duplicate', u'qid1', u'qid2', u'question1', u'question2',\n",
       "       u'test_id', u'1wl_tfidf_cosine', u'1wl_tfidf_l2_euclidean',\n",
       "       u'1wl_tf_l2_euclidean', u'm_w1l_tfidf_oof'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
