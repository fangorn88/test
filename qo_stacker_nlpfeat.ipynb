{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/ipykernel/__main__.py:22: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "/usr/lib/python2.7/site-packages/ipykernel/__main__.py:34: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "/usr/lib/python2.7/site-packages/ipykernel/__main__.py:36: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "/usr/lib/python2.7/site-packages/ipykernel/__main__.py:65: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n"
     ]
    }
   ],
   "source": [
    "from __future__ import  division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import gc\n",
    "\n",
    "x1 = pd.read_csv('train_features.csv')\n",
    "x2 = pd.read_csv('magicfeat_train.csv')\n",
    "x3 = pd.read_csv('fanokas.csv')\n",
    "x4 = pd.read_csv('owl_feat.csv')\n",
    "x5 = pd.read_csv('jacfeat.csv')\n",
    "\n",
    "col_to_drop = ['z_len1','z_len2','z_word_len1','z_word_len2','z_word_match']\n",
    "\n",
    "x4.drop(col_to_drop, inplace = True, axis = 1)\n",
    "\n",
    "xf = pd.concat([x1,x2.ix[:,2:],x3.ix[:,1:],x4.ix[:,9:],x5.ix[:,1:]], axis = 1)\n",
    "\n",
    "x1t = pd.read_csv('test_features.csv')\n",
    "x2t = pd.read_csv('magicfeat_test.csv')\n",
    "x3t = pd.read_csv('fanokas_test.csv')\n",
    "x4t = pd.read_csv('owl_feat_test.csv')\n",
    "x5t = pd.read_csv('jacfeat_test.csv')\n",
    "\n",
    "col_to_drop = ['z_len1','z_len2','z_word_len1','z_word_len2','z_word_match']\n",
    "\n",
    "x4t.drop(col_to_drop, inplace = True, axis = 1)\n",
    "\n",
    "xft = pd.concat([x1t,x2t.ix[:,2:],x3t.ix[:,1:],x4t.ix[:,6:],x5t.ix[:,1:]], axis = 1)\n",
    "\n",
    "X = xf.ix[:,2:]\n",
    "\n",
    "y = X['is_duplicate'].values\n",
    "\n",
    "X.drop('is_duplicate', inplace = True, axis = 1)\n",
    "\n",
    "col_to_drop = ['q1_hash','q2_hash']\n",
    "\n",
    "X.drop(col_to_drop, inplace = True, axis = 1)\n",
    "\n",
    "# sbtr = np.loadtxt(\"sbenchfeat_tsvd100_train.gz\", delimiter=\",\")\n",
    "rstr = np.loadtxt(\"russ_tr.gz\", delimiter=\",\")\n",
    "rspacetr = np.loadtxt(\"russpacy_tr.gz\", delimiter=\",\")\n",
    "\n",
    "# sbts = np.loadtxt(\"sbenchfeat_tsvd100_test.gz\", delimiter=\",\")\n",
    "rsts = np.loadtxt(\"russ_ts.gz\", delimiter=\",\")\n",
    "rspacets = np.loadtxt(\"russpacy_ts.gz\", delimiter=\",\")\n",
    "\n",
    "\n",
    "magic2tr = np.loadtxt(\"magic2_tr.gz\", delimiter=\",\")\n",
    "magic2ts = np.loadtxt(\"magic2_ts.gz\", delimiter=\",\")\n",
    "\n",
    "magic3tr = np.loadtxt(\"magic3_tr.gz\", delimiter=\",\")\n",
    "magic3ts = np.loadtxt(\"magic3_ts.gz\", delimiter=\",\")\n",
    "\n",
    "# x_train = np.column_stack((np.array(X),rstr[:,0:17],rstr[:,18:20],rspacetr[:,0:3],sbtr,magic2tr,magic3tr))\n",
    "\n",
    "# print x_train.shape\n",
    "\n",
    "xtest = xft.ix[:,2:]\n",
    "\n",
    "col_to_drop = ['q1_hash','q2_hash']\n",
    "xtest.drop(col_to_drop, inplace = True, axis = 1)\n",
    "\n",
    "# x_test = np.column_stack((np.array(xtest),rsts[:,0:17],rsts[:,18:20],rspacets[:,0:3],sbts,magic2ts,magic3ts))\n",
    "\n",
    "# print x_test.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xgbtr = np.loadtxt(\"train_stacker_xgb1.csv\", delimiter=\",\")\n",
    "xgbts = np.loadtxt(\"test_stacker_xgb1.csv\", delimiter=\",\")\n",
    "\n",
    "# lstmtr = np.loadtxt(\"train_stacker_lstm2.csv\", delimiter=\",\")\n",
    "# lstmts = np.loadtxt(\"test_stacker_lstm2.csv\", delimiter=\",\")\n",
    "\n",
    "lstmtr1 = np.loadtxt(\"train_stacker_lstm4.csv\", delimiter=\",\")\n",
    "lstmts1 = np.loadtxt(\"test_stacker_lstm4.csv\", delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mgtr = pd.read_csv('new_magic_train.csv')\n",
    "mgts = pd.read_csv('new_magic_test.csv')                \n",
    "\n",
    "pgtr = pd.read_csv('pagerank_train.csv')\n",
    "pgts = pd.read_csv('pagerank_test.csv')                \n",
    "\n",
    "# mgts.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# id_tr = np.array(range(0,x_train.shape[0]))\n",
    "# id_ts = np.array(range(0,x_test.shape[0]))\n",
    "\n",
    "xgb_metatr = np.loadtxt(\"2train_stacker_xgb1.csv\", delimiter=\",\")\n",
    "xgb_metats = np.loadtxt(\"2test_stacker_xgb1.csv\", delimiter=\",\")\n",
    "\n",
    "nn_metatr = np.array(pd.read_csv('2train_stacker_nn1.csv'))\n",
    "nn_metats = np.loadtxt(\"2test_stacker_nn1.gz\", delimiter=\",\")\n",
    "\n",
    "# id_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290, 64)\n",
      "(2345796, 64)\n"
     ]
    }
   ],
   "source": [
    "# nntr = np.array(pd.read_csv('train_stacker.csv'))\n",
    "# nnts = np.loadtxt(\"test_stacker_nn1.csv\", delimiter=\",\")\n",
    "\n",
    "# rftr = np.loadtxt(\"train_stacker_rf1.csv\", delimiter=\",\")\n",
    "# rfts = np.loadtxt(\"test_stacker_rf1.csv\", delimiter=\",\")\n",
    "\n",
    "# lrtr = np.loadtxt(\"train_stacker_lr1.csv\", delimiter=\",\")\n",
    "# lrts = np.loadtxt(\"test_stacker_lr1.csv\", delimiter=\",\")\n",
    "\n",
    "# nntr1 = np.array(pd.read_csv('train_stacker_nn2.csv'))\n",
    "# nnts1 = np.loadtxt(\"test_stacker_nn2.csv\", delimiter=\",\")\n",
    "\n",
    "# glvtr = np.loadtxt(\"train_stacker_glv1.csv\", delimiter=\",\")\n",
    "# glvts = np.loadtxt(\"test_stacker_glv1.csv\", delimiter=\",\")\n",
    "\n",
    "# lstmtr2 = np.loadtxt(\"train_stacker_lstm4t.csv\", delimiter=\",\")\n",
    "# lstmts2 = np.loadtxt(\"test_stacker_lstm4t.csv\", delimiter=\",\")\n",
    "\n",
    "# sgtr = np.loadtxt(\"train_stacker_sgd1.csv\", delimiter=\",\")\n",
    "# sgts = np.loadtxt(\"test_stacker_sgd1.csv\", delimiter=\",\")\n",
    "\n",
    "# lbmtr = np.loadtxt(\"train_stacker_lgbm1.csv\", delimiter=\",\")\n",
    "# lbmts = np.loadtxt(\"test_stacker_lgbm1.csv\", delimiter=\",\")\n",
    "\n",
    "# sbenchtr = np.loadtxt(\"train_stacker_sbench1.csv\", delimiter=\",\")\n",
    "# sbenchts = np.loadtxt(\"test_stacker_sbench1.csv\", delimiter=\",\")\n",
    "\n",
    "\n",
    "\n",
    "# x_train = np.column_stack((np.array(X),rstr[:,0:17],rstr[:,18:20],rspacetr[:,0:3],sbtr,magic2tr,magic3tr,sbenchtr,\n",
    "#                            np.array(mgtr.iloc[:,2]),xgbtr,nntr,rftr,lstmtr1,lrtr,nntr1,sgtr,glvtr,lstmtr2,pgtr))\n",
    "\n",
    "# x_test = np.column_stack((np.array(xtest),rsts[:,0:17],rsts[:,18:20],rspacets[:,0:3],sbts,magic2ts,magic3ts,sbenchts,\n",
    "#                           np.array(mgts.iloc[:,2]),xgbts,nnts,rfts,lstmts1,lrts,nnts1,sgts,glvts,lstmts2,pgts))\n",
    "x_train = np.loadtxt(\"lstm_feat_train1.gz\",  delimiter=\",\" )\n",
    "\n",
    "x_test = np.loadtxt(\"lstm_feat_test1.gz\",  delimiter=\",\")\n",
    "\n",
    "# x_train = np.column_stack((np.array(X),rstr[:,0:17],rstr[:,18:20],rspacetr[:,0:3],magic2tr,magic3tr,sbenchtr,lbmtr,nlp_tr,\n",
    "#                            np.array(mgtr.iloc[:,2]),xgbtr,nntr,rftr,lstmtr1,lrtr,nntr1,sgtr,glvtr,lstmtr2,pgtr))\n",
    "\n",
    "# x_test = np.column_stack((np.array(xtest),rsts[:,0:17],rsts[:,18:20],rspacets[:,0:3],magic2ts,magic3ts,sbenchts,lbmts,nlp_ts,\n",
    "#                           np.array(mgts.iloc[:,2]),xgbts,nnts,rfts,lstmts1,lrts,nnts1,sgts,glvts,lstmts2,pgts))\n",
    "\n",
    "\n",
    "print x_train.shape\n",
    "print x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"2train_feat.gz\", x_train, delimiter=\",\", fmt='%.6f')\n",
    "\n",
    "np.savetxt(\"2test_feat.gz\", x_test, delimiter=\",\", fmt='%.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n",
      "(323432, 64)\n",
      "(80858, 64)\n",
      "Oversampling started for proportion: 0.369026565089\n",
      "Oversampling done, new proportion: 0.191259632688\n",
      "Oversampling started for proportion: 0.369883004774\n",
      "Oversampling done, new proportion: 0.191181170815\n",
      "(323432, 64)\n",
      "(80858, 64)\n",
      "[0]\ttrain-logloss:0.673459\tvalid-logloss:0.673831\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 10 rounds.\n",
      "[100]\ttrain-logloss:0.431661\tvalid-logloss:0.454603\n",
      "[200]\ttrain-logloss:0.4125\tvalid-logloss:0.448471\n",
      "[300]\ttrain-logloss:0.396794\tvalid-logloss:0.444795\n",
      "[400]\ttrain-logloss:0.383041\tvalid-logloss:0.442417\n",
      "[500]\ttrain-logloss:0.370874\tvalid-logloss:0.440491\n",
      "[600]\ttrain-logloss:0.359029\tvalid-logloss:0.438965\n",
      "[700]\ttrain-logloss:0.348198\tvalid-logloss:0.437862\n",
      "[800]\ttrain-logloss:0.336623\tvalid-logloss:0.436656\n",
      "[900]\ttrain-logloss:0.325876\tvalid-logloss:0.435748\n",
      "[999]\ttrain-logloss:0.316437\tvalid-logloss:0.43508\n",
      "[0.43508048059927634]\n",
      "(323432, 64)\n",
      "(80858, 64)\n",
      "Oversampling started for proportion: 0.368834871008\n",
      "Oversampling done, new proportion: 0.191277675158\n",
      "Oversampling started for proportion: 0.370649781098\n",
      "Oversampling done, new proportion: 0.191112046372\n",
      "(323432, 64)\n",
      "(80858, 64)\n",
      "[0]\ttrain-logloss:0.673425\tvalid-logloss:0.67382\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 10 rounds.\n",
      "[100]\ttrain-logloss:0.431484\tvalid-logloss:0.453634\n",
      "[200]\ttrain-logloss:0.412441\tvalid-logloss:0.447303\n",
      "[300]\ttrain-logloss:0.397136\tvalid-logloss:0.443821\n",
      "[400]\ttrain-logloss:0.383036\tvalid-logloss:0.441199\n",
      "[500]\ttrain-logloss:0.37156\tvalid-logloss:0.439508\n",
      "[600]\ttrain-logloss:0.360203\tvalid-logloss:0.437923\n",
      "[700]\ttrain-logloss:0.349433\tvalid-logloss:0.436599\n",
      "[800]\ttrain-logloss:0.337823\tvalid-logloss:0.435334\n",
      "[900]\ttrain-logloss:0.327658\tvalid-logloss:0.43457\n",
      "[999]\ttrain-logloss:0.317746\tvalid-logloss:0.433864\n",
      "[0.43508048059927634, 0.43386410315473145]\n",
      "(323432, 64)\n",
      "(80858, 64)\n",
      "Oversampling started for proportion: 0.369363575651\n",
      "Oversampling done, new proportion: 0.191228070175\n",
      "Oversampling started for proportion: 0.368534962527\n",
      "Oversampling done, new proportion: 0.191306190054\n",
      "(323432, 64)\n",
      "(80858, 64)\n",
      "[0]\ttrain-logloss:0.673472\tvalid-logloss:0.673794\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 10 rounds.\n",
      "[100]\ttrain-logloss:0.430905\tvalid-logloss:0.45407\n",
      "[200]\ttrain-logloss:0.411059\tvalid-logloss:0.447704\n",
      "[300]\ttrain-logloss:0.397342\tvalid-logloss:0.444383\n",
      "[400]\ttrain-logloss:0.384741\tvalid-logloss:0.442129\n",
      "[500]\ttrain-logloss:0.371575\tvalid-logloss:0.440034\n",
      "[600]\ttrain-logloss:0.35898\tvalid-logloss:0.438392\n",
      "[700]\ttrain-logloss:0.347984\tvalid-logloss:0.436947\n",
      "[800]\ttrain-logloss:0.337188\tvalid-logloss:0.435996\n",
      "[900]\ttrain-logloss:0.326194\tvalid-logloss:0.4351\n",
      "[999]\ttrain-logloss:0.316648\tvalid-logloss:0.434272\n",
      "[0.43508048059927634, 0.43386410315473145, 0.43427240559237984]\n",
      "(323432, 64)\n",
      "(80858, 64)\n",
      "Oversampling started for proportion: 0.369338840931\n",
      "Oversampling done, new proportion: 0.191230569741\n",
      "Oversampling started for proportion: 0.368633901407\n",
      "Oversampling done, new proportion: 0.191297371883\n",
      "(323432, 64)\n",
      "(80858, 64)\n",
      "[0]\ttrain-logloss:0.673412\tvalid-logloss:0.673739\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 10 rounds.\n",
      "[100]\ttrain-logloss:0.431055\tvalid-logloss:0.454528\n",
      "[200]\ttrain-logloss:0.411307\tvalid-logloss:0.448437\n",
      "[300]\ttrain-logloss:0.396654\tvalid-logloss:0.444662\n",
      "[400]\ttrain-logloss:0.384554\tvalid-logloss:0.442317\n",
      "[500]\ttrain-logloss:0.371017\tvalid-logloss:0.439986\n",
      "[600]\ttrain-logloss:0.359528\tvalid-logloss:0.438551\n",
      "[700]\ttrain-logloss:0.348802\tvalid-logloss:0.437376\n",
      "[800]\ttrain-logloss:0.337774\tvalid-logloss:0.436196\n",
      "[900]\ttrain-logloss:0.328008\tvalid-logloss:0.435547\n",
      "[999]\ttrain-logloss:0.317634\tvalid-logloss:0.434853\n",
      "[0.43508048059927634, 0.43386410315473145, 0.43427240559237984, 0.43485254625299902]\n",
      "(323432, 64)\n",
      "(80858, 64)\n",
      "Oversampling started for proportion: 0.369425412451\n",
      "Oversampling done, new proportion: 0.191222435076\n",
      "Oversampling started for proportion: 0.368287615326\n",
      "Oversampling done, new proportion: 0.1913301037\n",
      "(323432, 64)\n",
      "(80858, 64)\n",
      "[0]\ttrain-logloss:0.673431\tvalid-logloss:0.673731\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 10 rounds.\n",
      "[100]\ttrain-logloss:0.430919\tvalid-logloss:0.454066\n",
      "[200]\ttrain-logloss:0.412805\tvalid-logloss:0.448336\n",
      "[300]\ttrain-logloss:0.399091\tvalid-logloss:0.445137\n",
      "[400]\ttrain-logloss:0.384554\tvalid-logloss:0.442141\n",
      "[500]\ttrain-logloss:0.371692\tvalid-logloss:0.440165\n",
      "[600]\ttrain-logloss:0.359748\tvalid-logloss:0.438767\n",
      "[700]\ttrain-logloss:0.349104\tvalid-logloss:0.437512\n",
      "[800]\ttrain-logloss:0.338449\tvalid-logloss:0.436443\n",
      "[900]\ttrain-logloss:0.328236\tvalid-logloss:0.43558\n",
      "[999]\ttrain-logloss:0.318356\tvalid-logloss:0.434879\n",
      "[0.43508048059927634, 0.43386410315473145, 0.43427240559237984, 0.43485254625299902, 0.43487928001608517]\n"
     ]
    }
   ],
   "source": [
    "RS = 2016\n",
    "ROUNDS = 400\n",
    "\n",
    "print(\"Started\")\n",
    "np.random.seed(RS)\n",
    "input_folder = ''\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set our parameters for xgboost\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.05\n",
    "params['max_depth'] = 9\n",
    "params['seed'] = RS\n",
    "params['gamma'] = 0.5\n",
    "params['subsample'] = 0.75\n",
    "params['colsample_bytree'] = 0.75\n",
    "params['min_child_weight'] = 10\n",
    "params['reg_alpha'] = 2\n",
    "params['n_jobs'] = 12\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "y_train = np.array(y)\n",
    "\n",
    "train_stacker=[ [0.0 for s in range(1)]  for k in range (0,(x_train.shape[0])) ]\n",
    "\n",
    "cv_scores = []\n",
    "oof_preds = []\n",
    "a = [0 for x in range(2345796)]\n",
    "# StratifiedKFold\n",
    "# kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=RS)\n",
    "# for dev_index, val_index in kf.split(range(x_train.shape[0]),y_train):\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2016)\n",
    "for dev_index, val_index in kf.split(range(x_train.shape[0])):\n",
    "        dev_X, val_X = x_train[dev_index,:], x_train[val_index,:]\n",
    "        dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "        print dev_X.shape\n",
    "        print val_X.shape\n",
    "        \n",
    "        pos_train = dev_X[dev_y == 1]\n",
    "        neg_train = dev_X[dev_y == 0]\n",
    "\n",
    "        print(\"Oversampling started for proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
    "        p = 0.165\n",
    "        scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "        while scale > 1:\n",
    "            neg_train = np.concatenate((neg_train, neg_train))\n",
    "            scale -=1\n",
    "        neg_train = np.concatenate((neg_train, neg_train[:int(scale * len(neg_train))]))\n",
    "        print(\"Oversampling done, new proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
    "\n",
    "        Xd = np.concatenate((pos_train, neg_train))\n",
    "        yd = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "        del pos_train, neg_train  \n",
    "\n",
    "        pos_train = val_X[val_y == 1]\n",
    "        neg_train = val_X[val_y == 0]\n",
    "\n",
    "        print(\"Oversampling started for proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
    "        p = 0.165\n",
    "        scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "        while scale > 1:\n",
    "            neg_train = np.concatenate((neg_train, neg_train))\n",
    "            scale -=1\n",
    "        neg_train = np.concatenate((neg_train, neg_train[:int(scale * len(neg_train))]))\n",
    "        print(\"Oversampling done, new proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
    "\n",
    "        Xv = np.concatenate((pos_train, neg_train))\n",
    "        yv = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "        del pos_train, neg_train  \n",
    "\n",
    "        print dev_X.shape\n",
    "        print val_X.shape\n",
    "\n",
    "        d_train = xgb.DMatrix(Xd, label=yd)\n",
    "        d_valid = xgb.DMatrix(Xv, label=yv)\n",
    "\n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "        bst = xgb.train(params, d_train, 1000, watchlist, early_stopping_rounds=10, verbose_eval=100)\n",
    "        # ntree_limit=model.best_ntree_limit\n",
    "        preds = bst.predict(d_valid, ntree_limit=bst.best_ntree_limit)\n",
    "        cv_scores.append(log_loss(yv, preds))\n",
    "        print(cv_scores)\n",
    "#         break\n",
    "        \n",
    "        d_test = xgb.DMatrix(x_test)\n",
    "        preds_tr = bst.predict(d_test, ntree_limit=bst.best_ntree_limit)\n",
    "\n",
    "        a = np.column_stack((a,preds_tr))\n",
    "\n",
    "        d_valorg = xgb.DMatrix(val_X, label=val_y)\n",
    "        predsorg = bst.predict(d_valorg, ntree_limit=bst.best_ntree_limit)\n",
    "\n",
    "#         predictions = preds.reshape(-1,1)\n",
    "        no=0\n",
    "        for real_index in val_index:\n",
    "            for d in range (0,1):\n",
    "                train_stacker[real_index][d]=(predsorg[no])\n",
    "            no+=1\n",
    "\n",
    "# [0.15360504630455896, 0.15447078993338806, 0.15552687734108414, 0.15476172066330016, 0.15241640738176887]\n",
    "# [0.15311663729837133, 0.15390724875863718, 0.15488412233071547, 0.15409576186936139, 0.15184966391506896]\n",
    "# [0.15263729582890673, 0.15299274474295882, 0.15413732150213838, 0.15329555282049126, 0.15168823883360641]\n",
    "# [0.15211336771315526, 0.15263665115074093, 0.15316278739909817, 0.15283526907831582, 0.15115565771008421]\n",
    "# [0.14679638005158449, 0.14643568394088646, 0.14695301106657524, 0.14738008193419261, 0.14605956315322077]\n",
    "# [0.14629926832710563, 0.14606952289254052, 0.14638265721616533, 0.14687541244637101, 0.14573516873536307]\n",
    "# [0.14655595844865862, 0.14596795960342543, 0.14643300162670578]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "b = pd.DataFrame(a)\n",
    "\n",
    "b['sum'] = b.sum(axis = 1)/5\n",
    "\n",
    "np.savetxt(\"train_stacker_conv1.gz\", train_stacker, delimiter=\",\", fmt='%.6f')\n",
    "\n",
    "np.savetxt(\"test_stacker_conv1.gz\", np.array(b['sum']), delimiter=\",\", fmt='%.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = x4t['test_id']\n",
    "sub['is_duplicate'] = b['sum']\n",
    "sub.to_csv(\"xgb2016_xgbnnlstm_14.csv\", index=False)\n",
    "# [0.17016885873458976][0.17057354196031635]\n",
    "# [0.14779218706406064, 0.14736209573863371, 0.14791776295751807, 0.14854973453415368, 0.14707683432508603]\n",
    "# [0.16395319948454035, 0.16342681680227539, 0.1645509607385589, 0.16476062285669904, 0.16366588193787229]\n",
    "# [0.16391499039074689, 0.16310379978781103, 0.1643765694535835, 0.16461148761217967, 0.16346322504582803]\n",
    "# [0.15594182184782154, 0.15636525572885629]\n",
    "# [0.15601431264055107, 0.15649749323218259, 0.15664772713794661, 0.15711803971832813, 0.15506936911164648] - md9\n",
    "# [0.15566405419387641, 0.15630713900809348, 0.15670020109376631, 0.15657009005989708, 0.15495084710684223] - md9n\n",
    "# [0.15554099102519406, 0.15602266978895404, 0.15628836502397056, 0.15601721137359009, 0.15455359581827885] - md7\n",
    "# [0.15558689011874977, 0.15590592436387529, 0.1564077030690208, 0.15590118071950423, 0.15445916419880135] - md6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"2train_stacker_xgb2.gz\", train_stacker, delimiter=\",\", fmt='%.6f')\n",
    "\n",
    "np.savetxt(\"2test_stacker_xgb2.gz\", np.array(b['sum']), delimiter=\",\", fmt='%.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "# 'metric': 'binary_logloss', 'num_boost_round' :1000,\n",
    "t4_params = {\n",
    "    'boosting_type': 'gbdt', 'objective': 'binary', 'nthread': 12, 'silent': True,\n",
    "    'num_leaves': 2**6, 'learning_rate': 0.05, 'max_depth': 9,\n",
    "    'max_bin': 255, 'subsample_for_bin': 50000,\n",
    "    'subsample': 0.85, 'subsample_freq': 1, 'colsample_bytree': 0.80, 'reg_alpha':2, 'reg_lambda':0,\n",
    "    'min_split_gain': 0.5, 'min_child_weight': 10, 'min_child_samples': 2, 'scale_pos_weight': 1}\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(323432, 126)\n",
      "(80858, 126)\n",
      "Oversampling started for proportion: 0.369026565089\n",
      "Oversampling done, new proportion: 0.191259632688\n",
      "Oversampling started for proportion: 0.369883004774\n",
      "Oversampling done, new proportion: 0.191181170815\n",
      "(323432, 126)\n",
      "(80858, 126)\n",
      "Train until valid scores didn't improve in 10 rounds.\n",
      "[25]\tvalid_0's binary_logloss: 0.258373\n",
      "[50]\tvalid_0's binary_logloss: 0.175423\n",
      "[75]\tvalid_0's binary_logloss: 0.154979\n",
      "[100]\tvalid_0's binary_logloss: 0.149519\n",
      "[125]\tvalid_0's binary_logloss: 0.147938\n",
      "[150]\tvalid_0's binary_logloss: 0.147312\n",
      "[175]\tvalid_0's binary_logloss: 0.146933\n",
      "[200]\tvalid_0's binary_logloss: 0.146794\n",
      "[225]\tvalid_0's binary_logloss: 0.146751\n",
      "[250]\tvalid_0's binary_logloss: 0.146704\n",
      "[275]\tvalid_0's binary_logloss: 0.146604\n",
      "Early stopping, best iteration is:\n",
      "[273]\tvalid_0's binary_logloss: 0.146596\n",
      "[0.1465937463512855]\n",
      "(323432, 126)\n",
      "(80858, 126)\n",
      "Oversampling started for proportion: 0.368834871008\n",
      "Oversampling done, new proportion: 0.191277675158\n",
      "Oversampling started for proportion: 0.370649781098\n",
      "Oversampling done, new proportion: 0.191112046372\n",
      "(323432, 126)\n",
      "(80858, 126)\n",
      "Train until valid scores didn't improve in 10 rounds.\n",
      "[25]\tvalid_0's binary_logloss: 0.259316\n",
      "[50]\tvalid_0's binary_logloss: 0.176054\n",
      "[75]\tvalid_0's binary_logloss: 0.155222\n",
      "[100]\tvalid_0's binary_logloss: 0.149387\n",
      "[125]\tvalid_0's binary_logloss: 0.147514\n",
      "[150]\tvalid_0's binary_logloss: 0.146849\n",
      "[175]\tvalid_0's binary_logloss: 0.146497\n",
      "[200]\tvalid_0's binary_logloss: 0.146333\n",
      "[225]\tvalid_0's binary_logloss: 0.146236\n",
      "[250]\tvalid_0's binary_logloss: 0.146187\n",
      "Early stopping, best iteration is:\n",
      "[244]\tvalid_0's binary_logloss: 0.14616\n",
      "[0.1465937463512855, 0.14615366345193292]\n",
      "(323432, 126)\n",
      "(80858, 126)\n",
      "Oversampling started for proportion: 0.369363575651\n",
      "Oversampling done, new proportion: 0.191228070175\n",
      "Oversampling started for proportion: 0.368534962527\n",
      "Oversampling done, new proportion: 0.191306190054\n",
      "(323432, 126)\n",
      "(80858, 126)\n",
      "Train until valid scores didn't improve in 10 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.176862\n",
      "[75]\tvalid_0's binary_logloss: 0.156151\n",
      "[100]\tvalid_0's binary_logloss: 0.150283\n",
      "[125]\tvalid_0's binary_logloss: 0.148393\n",
      "[150]\tvalid_0's binary_logloss: 0.147642\n",
      "[175]\tvalid_0's binary_logloss: 0.147264\n",
      "[200]\tvalid_0's binary_logloss: 0.147109\n",
      "[225]\tvalid_0's binary_logloss: 0.146995\n",
      "[250]\tvalid_0's binary_logloss: 0.146938\n",
      "Early stopping, best iteration is:\n",
      "[258]\tvalid_0's binary_logloss: 0.146919\n",
      "[0.1465937463512855, 0.14615366345193292, 0.1469229977170145]\n",
      "(323432, 126)\n",
      "(80858, 126)\n",
      "Oversampling started for proportion: 0.369338840931\n",
      "Oversampling done, new proportion: 0.191230569741\n",
      "Oversampling started for proportion: 0.368633901407\n",
      "Oversampling done, new proportion: 0.191297371883\n",
      "(323432, 126)\n",
      "(80858, 126)\n",
      "Train until valid scores didn't improve in 10 rounds.\n",
      "[25]\tvalid_0's binary_logloss: 0.25996\n",
      "[50]\tvalid_0's binary_logloss: 0.176804\n",
      "[75]\tvalid_0's binary_logloss: 0.155955\n",
      "[100]\tvalid_0's binary_logloss: 0.15013\n",
      "[125]\tvalid_0's binary_logloss: 0.148346\n",
      "[150]\tvalid_0's binary_logloss: 0.147641\n",
      "[175]\tvalid_0's binary_logloss: 0.147244\n",
      "[200]\tvalid_0's binary_logloss: 0.147102\n",
      "[225]\tvalid_0's binary_logloss: 0.147014\n",
      "Early stopping, best iteration is:\n",
      "[232]\tvalid_0's binary_logloss: 0.146996\n",
      "[0.1465937463512855, 0.14615366345193292, 0.1469229977170145, 0.14700542733023864]\n",
      "(323432, 126)\n",
      "(80858, 126)\n",
      "Oversampling started for proportion: 0.369425412451\n",
      "Oversampling done, new proportion: 0.191222435076\n",
      "Oversampling started for proportion: 0.368287615326\n",
      "Oversampling done, new proportion: 0.1913301037\n",
      "(323432, 126)\n",
      "(80858, 126)\n",
      "Train until valid scores didn't improve in 10 rounds.\n",
      "[25]\tvalid_0's binary_logloss: 0.258632\n",
      "[50]\tvalid_0's binary_logloss: 0.175077\n",
      "[75]\tvalid_0's binary_logloss: 0.154257\n",
      "[100]\tvalid_0's binary_logloss: 0.148607\n",
      "[125]\tvalid_0's binary_logloss: 0.146855\n",
      "[150]\tvalid_0's binary_logloss: 0.146329\n",
      "[175]\tvalid_0's binary_logloss: 0.146084\n",
      "[200]\tvalid_0's binary_logloss: 0.146035\n",
      "[225]\tvalid_0's binary_logloss: 0.145933\n",
      "Early stopping, best iteration is:\n",
      "[225]\tvalid_0's binary_logloss: 0.145933\n",
      "[0.1465937463512855, 0.14615366345193292, 0.1469229977170145, 0.14700542733023864, 0.14592327833231394]\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(y)\n",
    "\n",
    "train_stacker=[ [0.0 for s in range(1)]  for k in range (0,(x_train.shape[0])) ]\n",
    "\n",
    "cv_scores = []\n",
    "oof_preds = []\n",
    "a = [0 for x in range(2345796)]\n",
    "# StratifiedKFold\n",
    "# kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=RS)\n",
    "# for dev_index, val_index in kf.split(range(x_train.shape[0]),y_train):\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2016)\n",
    "for dev_index, val_index in kf.split(range(x_train.shape[0])):\n",
    "        dev_X, val_X = x_train[dev_index,:], x_train[val_index,:]\n",
    "        dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "        print dev_X.shape\n",
    "        print val_X.shape\n",
    "        \n",
    "        pos_train = dev_X[dev_y == 1]\n",
    "        neg_train = dev_X[dev_y == 0]\n",
    "\n",
    "        print(\"Oversampling started for proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
    "        p = 0.165\n",
    "        scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "        while scale > 1:\n",
    "            neg_train = np.concatenate((neg_train, neg_train))\n",
    "            scale -=1\n",
    "        neg_train = np.concatenate((neg_train, neg_train[:int(scale * len(neg_train))]))\n",
    "        print(\"Oversampling done, new proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
    "\n",
    "        Xd = np.concatenate((pos_train, neg_train))\n",
    "        yd = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "        del pos_train, neg_train  \n",
    "\n",
    "        pos_train = val_X[val_y == 1]\n",
    "        neg_train = val_X[val_y == 0]\n",
    "\n",
    "        print(\"Oversampling started for proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
    "        p = 0.165\n",
    "        scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "        while scale > 1:\n",
    "            neg_train = np.concatenate((neg_train, neg_train))\n",
    "            scale -=1\n",
    "        neg_train = np.concatenate((neg_train, neg_train[:int(scale * len(neg_train))]))\n",
    "        print(\"Oversampling done, new proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
    "\n",
    "        Xv = np.concatenate((pos_train, neg_train))\n",
    "        yv = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "        del pos_train, neg_train  \n",
    "\n",
    "        print dev_X.shape\n",
    "        print val_X.shape\n",
    "\n",
    "        t4 = lgbm.sklearn.LGBMClassifier(n_estimators=1000, seed=2016, **t4_params)\n",
    "        bst = t4.fit(Xd, yd, \n",
    "                       eval_set = [(Xv,yv)], eval_metric = 'logloss',early_stopping_rounds = 10, verbose =25) \n",
    "\n",
    "        preds = bst.predict_proba(Xv)\n",
    "        cv_scores.append(log_loss(yv, preds))\n",
    "\n",
    "        preds_tr = bst.predict_proba(x_test)\n",
    "\n",
    "        a = np.column_stack((a,preds_tr[:,1]))\n",
    "        print(cv_scores)\n",
    "\n",
    "        predsorg = bst.predict_proba(val_X)\n",
    "\n",
    "#         predictions = preds.reshape(-1,1)\n",
    "        no=0\n",
    "        for real_index in val_index:\n",
    "            for d in range (0,1):\n",
    "                train_stacker[real_index][d]=(predsorg[no][1])\n",
    "            no+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# [0.14679638005158449, 0.14643568394088646, 0.14695301106657524, 0.14738008193419261, 0.14605956315322077]\n",
    "# [0.14629926832710563, 0.14606952289254052, 0.14638265721616533, 0.14687541244637101, 0.14573516873536307]\n",
    "# [0.1465937463512855, 0.14615366345193292, 0.1469229977170145, 0.14700542733023864, 0.14592327833231394]\n",
    "b = pd.DataFrame(a)\n",
    "b['sum'] = b.sum(axis = 1)/5\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = x4t['test_id']\n",
    "sub['is_duplicate'] = b['sum']\n",
    "sub.to_csv(\"lgbm_xgbnnlstm_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.231833</td>\n",
       "      <td>0.201949</td>\n",
       "      <td>0.193909</td>\n",
       "      <td>0.186294</td>\n",
       "      <td>0.187436</td>\n",
       "      <td>0.200284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.109984</td>\n",
       "      <td>0.107898</td>\n",
       "      <td>0.103193</td>\n",
       "      <td>0.107063</td>\n",
       "      <td>0.117343</td>\n",
       "      <td>0.109096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007783</td>\n",
       "      <td>0.007487</td>\n",
       "      <td>0.009395</td>\n",
       "      <td>0.008523</td>\n",
       "      <td>0.008066</td>\n",
       "      <td>0.008251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.000624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999278</td>\n",
       "      <td>0.998631</td>\n",
       "      <td>0.998777</td>\n",
       "      <td>0.999252</td>\n",
       "      <td>0.999004</td>\n",
       "      <td>0.998988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.689839</td>\n",
       "      <td>0.794522</td>\n",
       "      <td>0.727547</td>\n",
       "      <td>0.655012</td>\n",
       "      <td>0.772338</td>\n",
       "      <td>0.727852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.255666</td>\n",
       "      <td>0.263397</td>\n",
       "      <td>0.251105</td>\n",
       "      <td>0.279102</td>\n",
       "      <td>0.286307</td>\n",
       "      <td>0.267115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.000420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0         1         2         3         4         5       sum\n",
       "0  0.0  0.000281  0.000371  0.000376  0.000351  0.000251  0.000326\n",
       "1  0.0  0.231833  0.201949  0.193909  0.186294  0.187436  0.200284\n",
       "2  0.0  0.109984  0.107898  0.103193  0.107063  0.117343  0.109096\n",
       "3  0.0  0.000133  0.000308  0.000197  0.000124  0.000201  0.000193\n",
       "4  0.0  0.007783  0.007487  0.009395  0.008523  0.008066  0.008251\n",
       "5  0.0  0.000567  0.000760  0.000791  0.000584  0.000418  0.000624\n",
       "6  0.0  0.999278  0.998631  0.998777  0.999252  0.999004  0.998988\n",
       "7  0.0  0.689839  0.794522  0.727547  0.655012  0.772338  0.727852\n",
       "8  0.0  0.255666  0.263397  0.251105  0.279102  0.286307  0.267115\n",
       "9  0.0  0.000376  0.000559  0.000431  0.000320  0.000413  0.000420"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
