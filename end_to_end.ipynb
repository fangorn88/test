{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nature Conservancy Fish Classification - End-to-End Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import GlobalAveragePooling2D, Activation, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "from utils import * \n",
    "from models import Vgg16BN, Inception, Resnet50\n",
    "from glob import iglob\n",
    "\n",
    "ROOT_DIR = os.getcwd()\n",
    "DATA_HOME_DIR = ROOT_DIR + '/data'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# paths\n",
    "data_path = DATA_HOME_DIR\n",
    "split_train_path = data_path + '/nof_excl/train/'\n",
    "valid_path = data_path + '/nof_excl/valid/'\n",
    "test_path = DATA_HOME_DIR + '/test/'\n",
    "saved_model_path = ROOT_DIR + '/models/end_to_end/'\n",
    "fish_detector_path = ROOT_DIR + '/models/fish_detector_480x270/0.03-loss_2epoch_480x270_0.3-dropout_0.001-lr_vggbn.h5'\n",
    "submission_path = ROOT_DIR + '/submissions/end_to_end/'\n",
    "\n",
    "# data\n",
    "batch_size = 8\n",
    "im_size = (270, 480)  # ht, wt (only 299x299 for inception)\n",
    "nb_split_train_samples = 2944\n",
    "nb_valid_samples = 395\n",
    "nb_test_samples = 1000\n",
    "classes = [\"ALB\", \"BET\", \"DOL\", \"LAG\", \"OTHER\", \"SHARK\", \"YFT\"]  # excluding \"NoF\"\n",
    "nb_classes = len(classes)\n",
    "\n",
    "# model\n",
    "nb_runs = 5\n",
    "nb_epoch = 30\n",
    "nb_aug = 5\n",
    "dropout = 0.4\n",
    "clip = 0.01\n",
    "archs = [\"vggbn\"]\n",
    "\n",
    "models = {\n",
    "    \"vggbn\": Vgg16BN(size=im_size, n_classes=nb_classes, lr=0.001,\n",
    "                           batch_size=batch_size, dropout=dropout),\n",
    "    \"inception\": Inception(size=(299, 299), n_classes=nb_classes,\n",
    "                           lr=0.001, batch_size=batch_size),\n",
    "    \"resnet\": Resnet50(size=im_size, n_classes=nb_classes, lr=0.001,\n",
    "                    batch_size=batch_size, dropout=dropout)\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build & Train Species Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier is looking at fish classes only (excludes \"NoF\" class). When we make predictions on the test set later, we'll first use our fish detection model to separate out \"NoF\", and then predict species from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(parent_model, model_str):\n",
    "    parent_model.build()    \n",
    "    model_fn = saved_model_path + '{val_loss:.2f}-loss_{epoch}epoch_' + model_str\n",
    "    ckpt = ModelCheckpoint(filepath=model_fn, monitor='val_loss',\n",
    "                           save_best_only=True, save_weights_only=True)\n",
    "    \n",
    "    parent_model.fit_val(split_train_path, valid_path, nb_trn_samples=nb_split_train_samples, \n",
    "                         nb_val_samples=nb_valid_samples, nb_epoch=nb_epoch, callbacks=[ckpt], aug=nb_aug)\n",
    "\n",
    "    model_path = max(iglob(saved_model_path + '*.h5'), key=os.path.getctime)\n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_all():    \n",
    "    model_paths = {\n",
    "        \"vggbn\": [],\n",
    "        \"inception\": [],\n",
    "        'resnet': [],\n",
    "    }\n",
    "    \n",
    "    for run in range(nb_runs):\n",
    "        print(\"Starting Training Run {0} of {1}...\\n\".format(run+1, nb_runs))\n",
    "        aug_str = \"aug\" if nb_aug else \"no-aug\"\n",
    "        \n",
    "        for arch in archs:\n",
    "            print(\"Training {} model...\\n\".format(arch))\n",
    "            model = models[arch]\n",
    "            model_str = \"{0}x{1}_{2}_{3}lr_run{4}_{5}.h5\".format(model.size[0], model.size[1], aug_str,\n",
    "                                                                 model.lr, run, arch)\n",
    "            model_path = train(model, model_str)\n",
    "            model_paths[arch].append(model_path)\n",
    "        \n",
    "    print(\"Done.\") \n",
    "    return model_paths\n",
    "        \n",
    "model_paths = train_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_preds(model_paths):\n",
    "    \n",
    "    predictions_full = np.zeros((nb_test_samples, nb_classes+1))\n",
    "    \n",
    "    for run in range(nb_runs):\n",
    "        print(\"\\nStarting Prediction Run {0} of {1}...\\n\".format(run+1, nb_runs))\n",
    "        predictions_aug = np.zeros((nb_test_samples, nb_classes+1))\n",
    "        \n",
    "        for aug in range(nb_aug):\n",
    "            print(\"\\n--Predicting on Augmentation {0} of {1}...\\n\".format(aug+1, nb_aug))\n",
    "            predictions_mod = np.zeros((nb_test_samples, nb_classes+1))\n",
    "            \n",
    "            for arch in archs:\n",
    "                print(\"----Predicting on {} model...\".format(arch))\n",
    "                \n",
    "                parent = models[arch]\n",
    "                model = parent.build()\n",
    "                model.load_weights(model_paths[arch][run])\n",
    "                \n",
    "                fish_detector = Vgg16BN(size=im_size, n_classes=2, lr=0.001,\n",
    "                                       batch_size=batch_size, dropout=dropout)\n",
    "                fish_detector.build()\n",
    "                fish_detector.model.load_weights(fish_detector_path)\n",
    "                \n",
    "                nofish_prob, _ = fish_detector.test(test_path, nb_test_samples, aug=nb_aug)\n",
    "                nofish_prob = nofish_prob[:, 1]\n",
    "                \n",
    "                species_prob, filenames = parent.test(test_path, nb_test_samples, aug=nb_aug)\n",
    "                \n",
    "                pred = np.insert(species_prob, 4, nofish_prob, axis=1)\n",
    "                predictions_mod += pred\n",
    "            \n",
    "            predictions_mod /= len(archs)\n",
    "            predictions_aug += predictions_mod\n",
    "\n",
    "        predictions_aug /= nb_aug\n",
    "        predictions_full += predictions_aug\n",
    "    \n",
    "    predictions_full /= nb_runs\n",
    "    return predictions_full, filenames                     \n",
    "    \n",
    "predictions, filenames = generate_preds(model_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_predictions(predictions):\n",
    "    \"\"\"Weights predictions based on probability image contains a fish as predicted by fish detector model\"\"\"\n",
    "    no_fish = predictions[:, 4]\n",
    "    fish = np.delete(predictions, 4, axis=1)\n",
    "\n",
    "    weights = -1. * (no_fish - 1.)\n",
    "    weights = weights.reshape(1000, 1)\n",
    "\n",
    "    fish = weights * fish\n",
    "    preds = np.insert(fish, 4, no_fish, axis=1)\n",
    "\n",
    "    return preds\n",
    "\n",
    "predictions = weight_predictions(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Predictions to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_submission(predictions, filenames):\n",
    "    preds = np.clip(predictions, clip, 1-clip)\n",
    "    sub_fn = submission_path + '{0}epoch_{1}aug_{2}clip_{3}runs'.format(nb_epoch, nb_aug, clip, nb_runs)\n",
    "    \n",
    "    for arch in archs:\n",
    "        sub_fn += \"_{}\".format(arch)\n",
    "\n",
    "    with open(sub_fn + '.csv', 'w') as f:\n",
    "        print(\"Writing Predictions to CSV...\")\n",
    "        f.write('image,ALB,BET,DOL,LAG,NoF,OTHER,SHARK,YFT\\n')\n",
    "        for i, image_name in enumerate(filenames):\n",
    "            pred = ['%.6f' % p for p in preds[i, :]]\n",
    "            f.write('%s,%s\\n' % (os.path.basename(image_name), ','.join(pred)))\n",
    "        print(\"Done.\")\n",
    "\n",
    "write_submission(predictions, filenames)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
