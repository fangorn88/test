{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files imported\n",
      "Creating features\n"
     ]
    }
   ],
   "source": [
    "from __future__ import  division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from math import sin, cos, sqrt, atan2, radians, degrees, fabs\n",
    "import string as str\n",
    "import re\n",
    "from random import randint\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import model_selection \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print 'Files imported'\n",
    "\n",
    "#creating test_train and y_train\n",
    "y = train_df['is_duplicate'].values\n",
    "\n",
    "y_train = np.array(y)\n",
    "\n",
    "\n",
    "print 'Creating features' \n",
    "\n",
    "x_tr = np.loadtxt(\"x_train.gz\", delimiter=\",\")\n",
    "x_ts = np.loadtxt(\"x_test.gz\", delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "x_tr[~np.isfinite(x_tr)] = 0\n",
    "# x_test[~np.isfinite(x_test)] = 0\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators = 100, n_jobs =-1)\n",
    "clf = clf.fit(x_tr, y_train)\n",
    "clf.feature_importances_  \n",
    "\n",
    "model = SelectFromModel(clf,threshold = 'median', prefit=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404290, 106)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = model.transform(x_tr)\n",
    "\n",
    "X_new.shape               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n",
      "(323432, 106)\n",
      "(80858, 106)\n",
      "Oversampling started for proportion: 0.369026565089\n",
      "Oversampling done, new proportion: 0.191259632688\n",
      "Oversampling started for proportion: 0.369883004774\n",
      "Oversampling done, new proportion: 0.191181170815\n",
      "(624047, 106)\n",
      "(156438, 106)\n",
      "[0]\ttrain-logloss:0.618231\tvalid-logloss:0.61811\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-logloss:0.183504\tvalid-logloss:0.188552\n",
      "[700]\ttrain-logloss:0.143527\tvalid-logloss:0.176013\n",
      "[800]\ttrain-logloss:0.139219\tvalid-logloss:0.175555\n",
      "[900]\ttrain-logloss:0.135095\tvalid-logloss:0.175499\n",
      "[1000]\ttrain-logloss:0.131197\tvalid-logloss:0.175415\n",
      "[1100]\ttrain-logloss:0.12742\tvalid-logloss:0.175257\n",
      "[1200]\ttrain-logloss:0.123785\tvalid-logloss:0.175178\n",
      "Stopping. Best iteration:\n",
      "[1226]\ttrain-logloss:0.122829\tvalid-logloss:0.175126\n",
      "\n",
      "[0.17517136348937903]\n"
     ]
    }
   ],
   "source": [
    "RS = 2016\n",
    "ROUNDS = 400\n",
    "\n",
    "print(\"Started\")\n",
    "np.random.seed(RS)\n",
    "input_folder = ''\n",
    "\n",
    "# x_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RS)\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set our parameters for xgboost\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.11\n",
    "params['max_depth'] = 5\n",
    "params['seed'] = RS\n",
    "params['gamma'] = 0.5\n",
    "params['subsample'] = 0.75\n",
    "params['colsample_bytree'] = 0.75\n",
    "params['min_child_weight'] = 10\n",
    "params['reg_alpha'] = 2\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "x_train = X_new\n",
    "# y_train = np.array(y)\n",
    "\n",
    "\n",
    "train_stacker=[ [0.0 for s in range(1)]  for k in range (0,(x_train.shape[0])) ]\n",
    "\n",
    "cv_scores = []\n",
    "oof_preds = []\n",
    "a = [0 for x in range(2345796)]\n",
    "# StratifiedKFold\n",
    "# kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=RS)\n",
    "# for dev_index, val_index in kf.split(range(x_train.shape[0]),y_train):\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2016)\n",
    "for dev_index, val_index in kf.split(range(x_train.shape[0])):\n",
    "        dev_X, val_X = x_train[dev_index,:], x_train[val_index,:]\n",
    "        dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "        print dev_X.shape\n",
    "        print val_X.shape\n",
    "        \n",
    "        pos_train = dev_X[dev_y == 1]\n",
    "        neg_train = dev_X[dev_y == 0]\n",
    "\n",
    "        print(\"Oversampling started for proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
    "        p = 0.165\n",
    "        scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "        while scale > 1:\n",
    "            neg_train = np.concatenate((neg_train, neg_train))\n",
    "            scale -=1\n",
    "        neg_train = np.concatenate((neg_train, neg_train[:int(scale * len(neg_train))]))\n",
    "        print(\"Oversampling done, new proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
    "\n",
    "        dev_X = np.concatenate((pos_train, neg_train))\n",
    "        dev_y = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "        del pos_train, neg_train  \n",
    "\n",
    "        pos_train = val_X[val_y == 1]\n",
    "        neg_train = val_X[val_y == 0]\n",
    "\n",
    "        print(\"Oversampling started for proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
    "        p = 0.165\n",
    "        scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "        while scale > 1:\n",
    "            neg_train = np.concatenate((neg_train, neg_train))\n",
    "            scale -=1\n",
    "        neg_train = np.concatenate((neg_train, neg_train[:int(scale * len(neg_train))]))\n",
    "        print(\"Oversampling done, new proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
    "\n",
    "        val_X = np.concatenate((pos_train, neg_train))\n",
    "        val_y = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "        del pos_train, neg_train  \n",
    "\n",
    "        print dev_X.shape\n",
    "        print val_X.shape\n",
    "\n",
    "        d_train = xgb.DMatrix(dev_X, label=dev_y)\n",
    "        d_valid = xgb.DMatrix(val_X, label=val_y)\n",
    "\n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "        bst = xgb.train(params, d_train, 15000, watchlist, early_stopping_rounds=50, verbose_eval=100)\n",
    "\n",
    "        preds = bst.predict(d_valid)\n",
    "\n",
    "        cv_scores.append(log_loss(val_y, preds))\n",
    "        \n",
    "#         d_test = xgb.DMatrix(x_test)\n",
    "#         preds_tr = bst.predict(d_test)\n",
    "\n",
    "#         a = np.column_stack((a,preds_tr))\n",
    "#         cv_scores.append(log_loss(val_y, preds*0.99))\n",
    "#         cv_scores.append(log_loss(val_y, preds*0.98))\n",
    "#         cv_scores.append(log_loss(val_y, preds*0.95))\n",
    "#         cv_scores.append(log_loss(val_y, preds*0.90))\n",
    "\n",
    "        print(cv_scores)\n",
    "        break\n",
    "\n",
    "#         no=0\n",
    "#         for real_index in val_index:\n",
    "# #             for d in range (0,1):\n",
    "#             train_stacker[real_index]= preds\n",
    "#             no+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
